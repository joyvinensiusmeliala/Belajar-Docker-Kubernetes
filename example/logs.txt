
==> Audit <==
|--------------|-----------------|----------|------------|---------|---------------------|---------------------|
|   Command    |      Args       | Profile  |    User    | Version |     Start Time      |      End Time       |
|--------------|-----------------|----------|------------|---------|---------------------|---------------------|
| start        | --driver=docker | minikube | kubernetes | v1.33.1 | 07 Jul 24 13:37 WIB | 07 Jul 24 13:41 WIB |
| ip           |                 | minikube | kubernetes | v1.33.1 | 07 Jul 24 13:41 WIB | 07 Jul 24 13:41 WIB |
| ssh          |                 | minikube | kubernetes | v1.33.1 | 07 Jul 24 13:42 WIB | 07 Jul 24 13:44 WIB |
| service      | nginx --url     | minikube | kubernetes | v1.33.1 | 07 Jul 24 13:52 WIB | 07 Jul 24 13:52 WIB |
| start        |                 | minikube | kubernetes | v1.33.1 | 08 Jul 24 10:30 WIB |                     |
| start        | --driver=docker | minikube | kubernetes | v1.33.1 | 08 Jul 24 10:33 WIB | 08 Jul 24 10:34 WIB |
| service      | nginx --url     | minikube | kubernetes | v1.33.1 | 08 Jul 24 12:11 WIB | 08 Jul 24 12:11 WIB |
| service      | nginx --url     | minikube | kubernetes | v1.33.1 | 08 Jul 24 12:11 WIB | 08 Jul 24 12:11 WIB |
| service      | nginx --url     | minikube | kubernetes | v1.33.1 | 08 Jul 24 12:16 WIB | 08 Jul 24 12:16 WIB |
| start        | --driver=docker | minikube | kubernetes | v1.33.1 | 08 Jul 24 12:19 WIB | 08 Jul 24 12:19 WIB |
| start        | --driver=docker | minikube | kubernetes | v1.33.1 | 09 Jul 24 09:22 WIB | 09 Jul 24 09:23 WIB |
| start        | --driver=docker | minikube | kubernetes | v1.33.1 | 09 Jul 24 21:12 WIB | 09 Jul 24 21:12 WIB |
| start        | --driver=docker | minikube | kubernetes | v1.33.1 | 10 Jul 24 07:57 WIB | 10 Jul 24 07:58 WIB |
| start        | --driver=docker | minikube | kubernetes | v1.33.1 | 10 Jul 24 12:33 WIB | 10 Jul 24 12:34 WIB |
| update-check |                 | minikube | kubernetes | v1.33.1 | 10 Jul 24 12:40 WIB | 10 Jul 24 12:40 WIB |
| service      | nginx-service   | minikube | kubernetes | v1.33.1 | 10 Jul 24 14:59 WIB |                     |
|--------------|-----------------|----------|------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2024/07/10 12:33:58
Running on machine: kubernetes-VirtualBox
Binary: Built with gc go1.22.1 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0710 12:33:58.287904    3134 out.go:291] Setting OutFile to fd 1 ...
I0710 12:33:58.288090    3134 out.go:343] isatty.IsTerminal(1) = true
I0710 12:33:58.288094    3134 out.go:304] Setting ErrFile to fd 2...
I0710 12:33:58.288096    3134 out.go:343] isatty.IsTerminal(2) = true
I0710 12:33:58.288227    3134 root.go:338] Updating PATH: /home/kubernetes/.minikube/bin
W0710 12:33:58.288319    3134 root.go:314] Error reading config file at /home/kubernetes/.minikube/config/config.json: open /home/kubernetes/.minikube/config/config.json: no such file or directory
I0710 12:33:58.289484    3134 out.go:298] Setting JSON to false
I0710 12:33:58.291861    3134 start.go:129] hostinfo: {"hostname":"kubernetes-VirtualBox","uptime":2501,"bootTime":1720587137,"procs":207,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"6.5.0-41-generic","kernelArch":"x86_64","virtualizationSystem":"vbox","virtualizationRole":"guest","hostId":"cde847c4-d64c-4c1d-b96d-ec94569f0e7b"}
I0710 12:33:58.291920    3134 start.go:139] virtualization: vbox guest
I0710 12:33:58.296722    3134 out.go:177] 😄  minikube v1.33.1 on Ubuntu 22.04 (vbox/amd64)
I0710 12:33:58.306134    3134 notify.go:220] Checking for updates...
I0710 12:33:58.306505    3134 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0710 12:33:58.308125    3134 driver.go:392] Setting default libvirt URI to qemu:///system
I0710 12:33:58.367296    3134 docker.go:122] docker version: linux-27.0.3:Docker Engine - Community
I0710 12:33:58.367566    3134 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0710 12:33:59.086312    3134 info.go:266] docker info: {ID:db21668f-b313-47e6-934d-011182f5e00c Containers:2 ContainersRunning:0 ContainersPaused:0 ContainersStopped:2 Images:2 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:23 OomKillDisable:false NGoroutines:39 SystemTime:2024-07-10 12:33:59.060505417 +0700 WIB LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.5.0-41-generic OperatingSystem:Ubuntu 22.04.4 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:2 MemTotal:4097036288 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:kubernetes-VirtualBox Labels:[] ExperimentalBuild:false ServerVersion:27.0.3 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae71819c4f5e67bb4d5ae76a6b735f29cc25774e Expected:ae71819c4f5e67bb4d5ae76a6b735f29cc25774e} RuncCommit:{ID:v1.1.13-0-g58aa920 Expected:v1.1.13-0-g58aa920} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.15.1] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.28.1]] Warnings:<nil>}}
I0710 12:33:59.086378    3134 docker.go:295] overlay module found
I0710 12:33:59.091914    3134 out.go:177] ✨  Using the docker driver based on existing profile
I0710 12:33:59.097151    3134 start.go:297] selected driver: docker
I0710 12:33:59.097158    3134 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/kubernetes:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0710 12:33:59.097221    3134 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0710 12:33:59.097280    3134 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0710 12:33:59.240796    3134 info.go:266] docker info: {ID:db21668f-b313-47e6-934d-011182f5e00c Containers:2 ContainersRunning:0 ContainersPaused:0 ContainersStopped:2 Images:2 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:23 OomKillDisable:false NGoroutines:39 SystemTime:2024-07-10 12:33:59.217438186 +0700 WIB LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.5.0-41-generic OperatingSystem:Ubuntu 22.04.4 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:2 MemTotal:4097036288 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:kubernetes-VirtualBox Labels:[] ExperimentalBuild:false ServerVersion:27.0.3 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae71819c4f5e67bb4d5ae76a6b735f29cc25774e Expected:ae71819c4f5e67bb4d5ae76a6b735f29cc25774e} RuncCommit:{ID:v1.1.13-0-g58aa920 Expected:v1.1.13-0-g58aa920} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.15.1] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.28.1]] Warnings:<nil>}}
I0710 12:33:59.241068    3134 cni.go:84] Creating CNI manager for ""
I0710 12:33:59.241080    3134 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0710 12:33:59.241119    3134 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/kubernetes:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0710 12:33:59.245417    3134 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I0710 12:33:59.249550    3134 cache.go:121] Beginning downloading kic base image for docker with docker
I0710 12:33:59.253517    3134 out.go:177] 🚜  Pulling base image v0.0.44 ...
I0710 12:33:59.259054    3134 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0710 12:33:59.259107    3134 preload.go:147] Found local preload: /home/kubernetes/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4
I0710 12:33:59.259114    3134 cache.go:56] Caching tarball of preloaded images
I0710 12:33:59.259609    3134 preload.go:173] Found /home/kubernetes/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0710 12:33:59.259619    3134 cache.go:59] Finished verifying existence of preloaded tar for v1.30.0 on docker
I0710 12:33:59.259707    3134 profile.go:143] Saving config to /home/kubernetes/.minikube/profiles/minikube/config.json ...
I0710 12:33:59.259997    3134 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e in local docker daemon
I0710 12:33:59.283428    3134 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e in local docker daemon, skipping pull
I0710 12:33:59.283440    3134 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e exists in daemon, skipping load
I0710 12:33:59.283587    3134 cache.go:194] Successfully downloaded all kic artifacts
I0710 12:33:59.283623    3134 start.go:360] acquireMachinesLock for minikube: {Name:mk9c412b240344606568902136904d5ad7c63101 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0710 12:33:59.283843    3134 start.go:364] duration metric: took 129.436µs to acquireMachinesLock for "minikube"
I0710 12:33:59.283862    3134 start.go:96] Skipping create...Using existing machine configuration
I0710 12:33:59.283870    3134 fix.go:54] fixHost starting: 
I0710 12:33:59.284031    3134 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0710 12:33:59.310847    3134 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0710 12:33:59.310863    3134 fix.go:138] unexpected machine state, will restart: <nil>
I0710 12:33:59.316428    3134 out.go:177] 🔄  Restarting existing docker container for "minikube" ...
I0710 12:33:59.320310    3134 cli_runner.go:164] Run: docker start minikube
I0710 12:34:00.324076    3134 cli_runner.go:217] Completed: docker start minikube: (1.003745296s)
I0710 12:34:00.324122    3134 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0710 12:34:00.424370    3134 kic.go:430] container "minikube" state is running.
I0710 12:34:00.425311    3134 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0710 12:34:00.456217    3134 profile.go:143] Saving config to /home/kubernetes/.minikube/profiles/minikube/config.json ...
I0710 12:34:00.456588    3134 machine.go:94] provisionDockerMachine start ...
I0710 12:34:00.456646    3134 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0710 12:34:00.495184    3134 main.go:141] libmachine: Using SSH client type: native
I0710 12:34:00.498243    3134 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0710 12:34:00.498252    3134 main.go:141] libmachine: About to run SSH command:
hostname
I0710 12:34:00.500411    3134 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:46528->127.0.0.1:32768: read: connection reset by peer
I0710 12:34:03.727608    3134 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0710 12:34:03.727621    3134 ubuntu.go:169] provisioning hostname "minikube"
I0710 12:34:03.727663    3134 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0710 12:34:03.781816    3134 main.go:141] libmachine: Using SSH client type: native
I0710 12:34:03.781980    3134 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0710 12:34:03.781986    3134 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0710 12:34:04.080870    3134 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0710 12:34:04.080928    3134 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0710 12:34:04.138953    3134 main.go:141] libmachine: Using SSH client type: native
I0710 12:34:04.139155    3134 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0710 12:34:04.139162    3134 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0710 12:34:04.294076    3134 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0710 12:34:04.294105    3134 ubuntu.go:175] set auth options {CertDir:/home/kubernetes/.minikube CaCertPath:/home/kubernetes/.minikube/certs/ca.pem CaPrivateKeyPath:/home/kubernetes/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/kubernetes/.minikube/machines/server.pem ServerKeyPath:/home/kubernetes/.minikube/machines/server-key.pem ClientKeyPath:/home/kubernetes/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/kubernetes/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/kubernetes/.minikube}
I0710 12:34:04.294162    3134 ubuntu.go:177] setting up certificates
I0710 12:34:04.294170    3134 provision.go:84] configureAuth start
I0710 12:34:04.294214    3134 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0710 12:34:04.324577    3134 provision.go:143] copyHostCerts
I0710 12:34:04.326150    3134 exec_runner.go:144] found /home/kubernetes/.minikube/ca.pem, removing ...
I0710 12:34:04.326157    3134 exec_runner.go:203] rm: /home/kubernetes/.minikube/ca.pem
I0710 12:34:04.326192    3134 exec_runner.go:151] cp: /home/kubernetes/.minikube/certs/ca.pem --> /home/kubernetes/.minikube/ca.pem (1090 bytes)
I0710 12:34:04.327644    3134 exec_runner.go:144] found /home/kubernetes/.minikube/cert.pem, removing ...
I0710 12:34:04.327650    3134 exec_runner.go:203] rm: /home/kubernetes/.minikube/cert.pem
I0710 12:34:04.327672    3134 exec_runner.go:151] cp: /home/kubernetes/.minikube/certs/cert.pem --> /home/kubernetes/.minikube/cert.pem (1131 bytes)
I0710 12:34:04.328701    3134 exec_runner.go:144] found /home/kubernetes/.minikube/key.pem, removing ...
I0710 12:34:04.328936    3134 exec_runner.go:203] rm: /home/kubernetes/.minikube/key.pem
I0710 12:34:04.328970    3134 exec_runner.go:151] cp: /home/kubernetes/.minikube/certs/key.pem --> /home/kubernetes/.minikube/key.pem (1679 bytes)
I0710 12:34:04.330047    3134 provision.go:117] generating server cert: /home/kubernetes/.minikube/machines/server.pem ca-key=/home/kubernetes/.minikube/certs/ca.pem private-key=/home/kubernetes/.minikube/certs/ca-key.pem org=kubernetes.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0710 12:34:04.432488    3134 provision.go:177] copyRemoteCerts
I0710 12:34:04.432590    3134 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0710 12:34:04.432618    3134 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0710 12:34:04.453281    3134 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/kubernetes/.minikube/machines/minikube/id_rsa Username:docker}
I0710 12:34:04.559601    3134 ssh_runner.go:362] scp /home/kubernetes/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1090 bytes)
I0710 12:34:04.629313    3134 ssh_runner.go:362] scp /home/kubernetes/.minikube/machines/server.pem --> /etc/docker/server.pem (1192 bytes)
I0710 12:34:04.668269    3134 ssh_runner.go:362] scp /home/kubernetes/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0710 12:34:04.712514    3134 provision.go:87] duration metric: took 418.334678ms to configureAuth
I0710 12:34:04.712528    3134 ubuntu.go:193] setting minikube options for container-runtime
I0710 12:34:04.712641    3134 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0710 12:34:04.712672    3134 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0710 12:34:04.774822    3134 main.go:141] libmachine: Using SSH client type: native
I0710 12:34:04.774964    3134 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0710 12:34:04.774969    3134 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0710 12:34:04.938045    3134 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0710 12:34:04.938056    3134 ubuntu.go:71] root file system type: overlay
I0710 12:34:04.938152    3134 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0710 12:34:04.938195    3134 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0710 12:34:04.966080    3134 main.go:141] libmachine: Using SSH client type: native
I0710 12:34:04.966212    3134 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0710 12:34:04.966251    3134 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0710 12:34:05.132154    3134 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0710 12:34:05.132202    3134 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0710 12:34:05.162290    3134 main.go:141] libmachine: Using SSH client type: native
I0710 12:34:05.162461    3134 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0710 12:34:05.162473    3134 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0710 12:34:05.329564    3134 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0710 12:34:05.329582    3134 machine.go:97] duration metric: took 4.872985299s to provisionDockerMachine
I0710 12:34:05.329590    3134 start.go:293] postStartSetup for "minikube" (driver="docker")
I0710 12:34:05.329599    3134 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0710 12:34:05.329649    3134 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0710 12:34:05.329688    3134 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0710 12:34:05.364376    3134 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/kubernetes/.minikube/machines/minikube/id_rsa Username:docker}
I0710 12:34:05.472994    3134 ssh_runner.go:195] Run: cat /etc/os-release
I0710 12:34:05.481402    3134 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0710 12:34:05.481418    3134 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0710 12:34:05.481423    3134 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0710 12:34:05.481427    3134 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I0710 12:34:05.481436    3134 filesync.go:126] Scanning /home/kubernetes/.minikube/addons for local assets ...
I0710 12:34:05.482921    3134 filesync.go:126] Scanning /home/kubernetes/.minikube/files for local assets ...
I0710 12:34:05.483816    3134 start.go:296] duration metric: took 154.217476ms for postStartSetup
I0710 12:34:05.483863    3134 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0710 12:34:05.483887    3134 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0710 12:34:05.507474    3134 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/kubernetes/.minikube/machines/minikube/id_rsa Username:docker}
I0710 12:34:05.622102    3134 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0710 12:34:05.636177    3134 fix.go:56] duration metric: took 6.35230396s for fixHost
I0710 12:34:05.636191    3134 start.go:83] releasing machines lock for "minikube", held for 6.352341961s
I0710 12:34:05.636247    3134 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0710 12:34:05.662979    3134 ssh_runner.go:195] Run: cat /version.json
I0710 12:34:05.663010    3134 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0710 12:34:05.663454    3134 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0710 12:34:05.663499    3134 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0710 12:34:05.693211    3134 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/kubernetes/.minikube/machines/minikube/id_rsa Username:docker}
I0710 12:34:05.697534    3134 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/kubernetes/.minikube/machines/minikube/id_rsa Username:docker}
I0710 12:34:06.091561    3134 ssh_runner.go:195] Run: systemctl --version
I0710 12:34:06.137104    3134 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0710 12:34:06.150027    3134 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0710 12:34:06.195078    3134 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0710 12:34:06.195270    3134 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0710 12:34:06.214013    3134 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0710 12:34:06.214027    3134 start.go:494] detecting cgroup driver to use...
I0710 12:34:06.214052    3134 detect.go:199] detected "systemd" cgroup driver on host os
I0710 12:34:06.214285    3134 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0710 12:34:06.247304    3134 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0710 12:34:06.270079    3134 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0710 12:34:06.288111    3134 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I0710 12:34:06.288154    3134 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0710 12:34:06.306344    3134 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0710 12:34:06.325399    3134 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0710 12:34:06.344250    3134 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0710 12:34:06.361796    3134 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0710 12:34:06.379108    3134 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0710 12:34:06.395708    3134 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0710 12:34:06.412413    3134 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0710 12:34:06.429183    3134 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0710 12:34:06.451403    3134 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0710 12:34:06.465485    3134 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0710 12:34:06.566640    3134 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0710 12:34:06.693757    3134 start.go:494] detecting cgroup driver to use...
I0710 12:34:06.693790    3134 detect.go:199] detected "systemd" cgroup driver on host os
I0710 12:34:06.693824    3134 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0710 12:34:06.720645    3134 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0710 12:34:06.720682    3134 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0710 12:34:06.744010    3134 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0710 12:34:06.780453    3134 ssh_runner.go:195] Run: which cri-dockerd
I0710 12:34:06.793261    3134 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0710 12:34:06.815259    3134 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0710 12:34:06.860012    3134 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0710 12:34:07.005909    3134 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0710 12:34:07.155739    3134 docker.go:574] configuring docker to use "systemd" as cgroup driver...
I0710 12:34:07.155835    3134 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I0710 12:34:07.189170    3134 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0710 12:34:07.285945    3134 ssh_runner.go:195] Run: sudo systemctl restart docker
I0710 12:34:08.971493    3134 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.685526614s)
I0710 12:34:08.971559    3134 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0710 12:34:08.997514    3134 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0710 12:34:09.026422    3134 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0710 12:34:09.046014    3134 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0710 12:34:09.156206    3134 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0710 12:34:09.254145    3134 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0710 12:34:09.348044    3134 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0710 12:34:09.379134    3134 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0710 12:34:09.397280    3134 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0710 12:34:09.489084    3134 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0710 12:34:10.281927    3134 start.go:541] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0710 12:34:10.281970    3134 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0710 12:34:10.290956    3134 start.go:562] Will wait 60s for crictl version
I0710 12:34:10.291191    3134 ssh_runner.go:195] Run: which crictl
I0710 12:34:10.299683    3134 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0710 12:34:10.794953    3134 start.go:578] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  26.1.1
RuntimeApiVersion:  v1
I0710 12:34:10.795002    3134 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0710 12:34:11.208259    3134 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0710 12:34:11.266815    3134 out.go:204] 🐳  Preparing Kubernetes v1.30.0 on Docker 26.1.1 ...
I0710 12:34:11.266896    3134 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0710 12:34:11.288783    3134 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0710 12:34:11.297974    3134 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0710 12:34:11.321371    3134 kubeadm.go:877] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/kubernetes:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0710 12:34:11.321563    3134 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0710 12:34:11.321610    3134 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0710 12:34:11.352332    3134 docker.go:685] Got preloaded images: -- stdout --
nginx:latest
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0710 12:34:11.352341    3134 docker.go:615] Images already preloaded, skipping extraction
I0710 12:34:11.352382    3134 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0710 12:34:11.383326    3134 docker.go:685] Got preloaded images: -- stdout --
nginx:latest
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0710 12:34:11.383337    3134 cache_images.go:84] Images are preloaded, skipping loading
I0710 12:34:11.383389    3134 kubeadm.go:928] updating node { 192.168.49.2 8443 v1.30.0 docker true true} ...
I0710 12:34:11.383520    3134 kubeadm.go:940] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.30.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0710 12:34:11.383574    3134 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0710 12:34:12.217472    3134 cni.go:84] Creating CNI manager for ""
I0710 12:34:12.217484    3134 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0710 12:34:12.217490    3134 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0710 12:34:12.217502    3134 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.30.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0710 12:34:12.217628    3134 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.30.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0710 12:34:12.217753    3134 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.30.0
I0710 12:34:12.241119    3134 binaries.go:44] Found k8s binaries, skipping transfer
I0710 12:34:12.241163    3134 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0710 12:34:12.255839    3134 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0710 12:34:12.294478    3134 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0710 12:34:12.327948    3134 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2149 bytes)
I0710 12:34:12.359096    3134 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0710 12:34:12.371388    3134 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0710 12:34:12.394035    3134 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0710 12:34:12.497824    3134 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0710 12:34:12.534225    3134 certs.go:68] Setting up /home/kubernetes/.minikube/profiles/minikube for IP: 192.168.49.2
I0710 12:34:12.534331    3134 certs.go:194] generating shared ca certs ...
I0710 12:34:12.534346    3134 certs.go:226] acquiring lock for ca certs: {Name:mkd529e05ab21dd4c31d9fe34d58912302731cb9 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0710 12:34:12.534727    3134 certs.go:235] skipping valid "minikubeCA" ca cert: /home/kubernetes/.minikube/ca.key
I0710 12:34:12.538493    3134 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/kubernetes/.minikube/proxy-client-ca.key
I0710 12:34:12.538808    3134 certs.go:256] generating profile certs ...
I0710 12:34:12.538920    3134 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /home/kubernetes/.minikube/profiles/minikube/client.key
I0710 12:34:12.543993    3134 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /home/kubernetes/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0710 12:34:12.546761    3134 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /home/kubernetes/.minikube/profiles/minikube/proxy-client.key
I0710 12:34:12.546932    3134 certs.go:484] found cert: /home/kubernetes/.minikube/certs/ca-key.pem (1679 bytes)
I0710 12:34:12.546957    3134 certs.go:484] found cert: /home/kubernetes/.minikube/certs/ca.pem (1090 bytes)
I0710 12:34:12.546977    3134 certs.go:484] found cert: /home/kubernetes/.minikube/certs/cert.pem (1131 bytes)
I0710 12:34:12.546994    3134 certs.go:484] found cert: /home/kubernetes/.minikube/certs/key.pem (1679 bytes)
I0710 12:34:12.548488    3134 ssh_runner.go:362] scp /home/kubernetes/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0710 12:34:12.619623    3134 ssh_runner.go:362] scp /home/kubernetes/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0710 12:34:12.690378    3134 ssh_runner.go:362] scp /home/kubernetes/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0710 12:34:12.779420    3134 ssh_runner.go:362] scp /home/kubernetes/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0710 12:34:12.863868    3134 ssh_runner.go:362] scp /home/kubernetes/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0710 12:34:12.923127    3134 ssh_runner.go:362] scp /home/kubernetes/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0710 12:34:12.978784    3134 ssh_runner.go:362] scp /home/kubernetes/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0710 12:34:13.022774    3134 ssh_runner.go:362] scp /home/kubernetes/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0710 12:34:13.070518    3134 ssh_runner.go:362] scp /home/kubernetes/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0710 12:34:13.154525    3134 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0710 12:34:13.220490    3134 ssh_runner.go:195] Run: openssl version
I0710 12:34:13.259234    3134 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0710 12:34:13.283300    3134 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0710 12:34:13.294026    3134 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Jul  7 06:41 /usr/share/ca-certificates/minikubeCA.pem
I0710 12:34:13.297087    3134 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0710 12:34:13.310090    3134 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0710 12:34:13.333645    3134 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0710 12:34:13.348400    3134 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0710 12:34:13.365279    3134 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0710 12:34:13.383567    3134 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0710 12:34:13.399091    3134 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0710 12:34:13.425401    3134 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0710 12:34:13.442639    3134 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0710 12:34:13.479442    3134 kubeadm.go:391] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/kubernetes:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0710 12:34:13.479540    3134 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0710 12:34:13.576119    3134 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
W0710 12:34:13.610307    3134 kubeadm.go:404] apiserver tunnel failed: apiserver port not set
I0710 12:34:13.610316    3134 kubeadm.go:407] found existing configuration files, will attempt cluster restart
I0710 12:34:13.610325    3134 kubeadm.go:587] restartPrimaryControlPlane start ...
I0710 12:34:13.610356    3134 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0710 12:34:13.642948    3134 kubeadm.go:129] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0710 12:34:13.644294    3134 kubeconfig.go:125] found "minikube" server: "https://192.168.49.2:8443"
I0710 12:34:13.680239    3134 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0710 12:34:13.715083    3134 kubeadm.go:624] The running cluster does not require reconfiguration: 192.168.49.2
I0710 12:34:13.715104    3134 kubeadm.go:591] duration metric: took 104.776282ms to restartPrimaryControlPlane
I0710 12:34:13.715115    3134 kubeadm.go:393] duration metric: took 235.678953ms to StartCluster
I0710 12:34:13.715126    3134 settings.go:142] acquiring lock: {Name:mka11971fb630e91d5a58820a635a2a92bddaa7d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0710 12:34:13.715211    3134 settings.go:150] Updating kubeconfig:  /home/kubernetes/.kube/config
I0710 12:34:13.715645    3134 lock.go:35] WriteFile acquiring /home/kubernetes/.kube/config: {Name:mk579b9c8f315599fa35ffbce126834f3c6618da Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0710 12:34:13.715920    3134 start.go:234] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0710 12:34:13.722289    3134 out.go:177] 🔎  Verifying Kubernetes components...
I0710 12:34:13.717175    3134 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0710 12:34:13.717187    3134 addons.go:502] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false yakd:false]
I0710 12:34:13.726104    3134 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0710 12:34:13.726124    3134 addons.go:234] Setting addon storage-provisioner=true in "minikube"
W0710 12:34:13.726128    3134 addons.go:243] addon storage-provisioner should already be in state true
I0710 12:34:13.726178    3134 host.go:66] Checking if "minikube" exists ...
I0710 12:34:13.726695    3134 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0710 12:34:13.726878    3134 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0710 12:34:13.726946    3134 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0710 12:34:13.726961    3134 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0710 12:34:13.727084    3134 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0710 12:34:13.920624    3134 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0710 12:34:13.918115    3134 addons.go:234] Setting addon default-storageclass=true in "minikube"
W0710 12:34:13.921037    3134 addons.go:243] addon default-storageclass should already be in state true
I0710 12:34:13.921062    3134 host.go:66] Checking if "minikube" exists ...
I0710 12:34:13.921295    3134 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0710 12:34:13.930129    3134 addons.go:426] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0710 12:34:13.930238    3134 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0710 12:34:13.930286    3134 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0710 12:34:13.988056    3134 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/kubernetes/.minikube/machines/minikube/id_rsa Username:docker}
I0710 12:34:13.990777    3134 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0710 12:34:14.020833    3134 addons.go:426] installing /etc/kubernetes/addons/storageclass.yaml
I0710 12:34:14.020844    3134 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0710 12:34:14.020881    3134 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0710 12:34:14.075159    3134 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/kubernetes/.minikube/machines/minikube/id_rsa Username:docker}
I0710 12:34:14.099979    3134 api_server.go:52] waiting for apiserver process to appear ...
I0710 12:34:14.100055    3134 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0710 12:34:14.377744    3134 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0710 12:34:14.482796    3134 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0710 12:34:14.601346    3134 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0710 12:34:15.457450    3134 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.079687228s)
W0710 12:34:15.457466    3134 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0710 12:34:15.457488    3134 retry.go:31] will retry after 144.769503ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0710 12:34:15.457527    3134 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0710 12:34:15.457531    3134 retry.go:31] will retry after 303.142968ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0710 12:34:15.457567    3134 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0710 12:34:15.600811    3134 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0710 12:34:15.603361    3134 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0710 12:34:15.762082    3134 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0710 12:34:16.025308    3134 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0710 12:34:16.025325    3134 retry.go:31] will retry after 264.987806ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0710 12:34:16.100921    3134 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0710 12:34:16.226573    3134 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0710 12:34:16.226596    3134 retry.go:31] will retry after 391.620231ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0710 12:34:16.291145    3134 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0710 12:34:16.600176    3134 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0710 12:34:16.618739    3134 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0710 12:34:16.848904    3134 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0710 12:34:16.848958    3134 retry.go:31] will retry after 763.831814ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0710 12:34:16.849047    3134 api_server.go:72] duration metric: took 3.133109477s to wait for apiserver process to appear ...
I0710 12:34:16.849053    3134 api_server.go:88] waiting for apiserver healthz status ...
I0710 12:34:16.849070    3134 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0710 12:34:16.849786    3134 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
W0710 12:34:16.953784    3134 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0710 12:34:16.953801    3134 retry.go:31] will retry after 398.571552ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0710 12:34:17.353852    3134 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0710 12:34:17.354029    3134 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0710 12:34:17.355303    3134 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0710 12:34:17.613722    3134 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0710 12:34:17.655313    3134 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0710 12:34:17.655330    3134 retry.go:31] will retry after 1.04144629s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0710 12:34:17.807317    3134 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0710 12:34:17.807337    3134 retry.go:31] will retry after 815.665864ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0710 12:34:17.849141    3134 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0710 12:34:17.850286    3134 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0710 12:34:18.349725    3134 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0710 12:34:18.350250    3134 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0710 12:34:18.624480    3134 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0710 12:34:18.699475    3134 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0710 12:34:18.849989    3134 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0710 12:34:18.850556    3134 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
W0710 12:34:19.102764    3134 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0710 12:34:19.102782    3134 retry.go:31] will retry after 1.447756902s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0710 12:34:19.113182    3134 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0710 12:34:19.113200    3134 retry.go:31] will retry after 1.578365098s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0710 12:34:19.354922    3134 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0710 12:34:20.550786    3134 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0710 12:34:20.693292    3134 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0710 12:34:23.255953    3134 api_server.go:279] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0710 12:34:23.255970    3134 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0710 12:34:23.255984    3134 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0710 12:34:23.267389    3134 api_server.go:279] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0710 12:34:23.267400    3134 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0710 12:34:23.349200    3134 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0710 12:34:23.375466    3134 api_server.go:279] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0710 12:34:23.375482    3134 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0710 12:34:23.849445    3134 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0710 12:34:23.852973    3134 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0710 12:34:23.852985    3134 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0710 12:34:24.349815    3134 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0710 12:34:24.359810    3134 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0710 12:34:24.359826    3134 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0710 12:34:24.641387    3134 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (4.09053121s)
I0710 12:34:24.641554    3134 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (3.948252457s)
I0710 12:34:24.677204    3134 out.go:177] 🌟  Enabled addons: storage-provisioner, default-storageclass
I0710 12:34:24.680891    3134 addons.go:505] duration metric: took 10.963692989s for enable addons: enabled=[storage-provisioner default-storageclass]
I0710 12:34:24.849351    3134 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0710 12:34:24.882072    3134 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0710 12:34:24.882089    3134 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0710 12:34:25.349654    3134 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0710 12:34:25.355908    3134 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I0710 12:34:25.359226    3134 api_server.go:141] control plane version: v1.30.0
I0710 12:34:25.359246    3134 api_server.go:131] duration metric: took 8.510187721s to wait for apiserver health ...
I0710 12:34:25.359253    3134 system_pods.go:43] waiting for kube-system pods to appear ...
I0710 12:34:25.370853    3134 system_pods.go:59] 7 kube-system pods found
I0710 12:34:25.370868    3134 system_pods.go:61] "coredns-7db6d8ff4d-v795g" [12486494-f56a-4193-908e-b9cff81bdf23] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0710 12:34:25.370872    3134 system_pods.go:61] "etcd-minikube" [89bfe01b-71ed-4340-8463-90675e7aa892] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0710 12:34:25.370875    3134 system_pods.go:61] "kube-apiserver-minikube" [241dbb67-3cc1-453d-a91a-f3113f4401d5] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0710 12:34:25.370878    3134 system_pods.go:61] "kube-controller-manager-minikube" [3d3f26c0-9dff-4e53-9acb-6a6aac7adf76] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0710 12:34:25.370880    3134 system_pods.go:61] "kube-proxy-7c8fb" [4e581081-7447-41c5-b2c8-2e0bad001100] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0710 12:34:25.370886    3134 system_pods.go:61] "kube-scheduler-minikube" [9d29469a-152c-40bd-a9b2-e40f522ac993] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0710 12:34:25.370889    3134 system_pods.go:61] "storage-provisioner" [cb914914-1658-425e-aba1-12de52410286] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0710 12:34:25.370894    3134 system_pods.go:74] duration metric: took 11.637233ms to wait for pod list to return data ...
I0710 12:34:25.370916    3134 kubeadm.go:576] duration metric: took 11.654966004s to wait for: map[apiserver:true system_pods:true]
I0710 12:34:25.370936    3134 node_conditions.go:102] verifying NodePressure condition ...
I0710 12:34:25.381022    3134 node_conditions.go:122] node storage ephemeral capacity is 35292736Ki
I0710 12:34:25.381042    3134 node_conditions.go:123] node cpu capacity is 2
I0710 12:34:25.381060    3134 node_conditions.go:105] duration metric: took 10.122026ms to run NodePressure ...
I0710 12:34:25.381070    3134 start.go:240] waiting for startup goroutines ...
I0710 12:34:25.381187    3134 start.go:245] waiting for cluster config update ...
I0710 12:34:25.381196    3134 start.go:254] writing updated cluster config ...
I0710 12:34:25.382255    3134 ssh_runner.go:195] Run: rm -f paused
I0710 12:34:25.590139    3134 start.go:600] kubectl: 1.30.2, cluster: 1.30.0 (minor skew: 0)
I0710 12:34:25.597024    3134 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Jul 10 07:18:28 minikube dockerd[812]: time="2024-07-10T07:18:28.696601534Z" level=info msg="ignoring event" container=b530e8cdc2dc91cd9cc8bf74e70be33239f16fd2567c86dab442357628d1f1a0 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 10 07:18:28 minikube dockerd[812]: time="2024-07-10T07:18:28.789064646Z" level=info msg="ignoring event" container=6638977e97c35f085810f2a779f327747d6d074a0c1d9d5f692736b1cb5df5a2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 10 07:18:28 minikube dockerd[812]: time="2024-07-10T07:18:28.800836206Z" level=info msg="ignoring event" container=328d3adb059b8be64483fad3354e11673598f850f592df79095160d1def3cb29 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 10 07:18:29 minikube dockerd[812]: time="2024-07-10T07:18:29.785870642Z" level=info msg="ignoring event" container=6844064a1fc9265e09bfff43913fd7ff18b2b1f153db13d61d4ecaa7bb0dc72a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 10 07:18:29 minikube dockerd[812]: time="2024-07-10T07:18:29.808170625Z" level=info msg="ignoring event" container=0d3a13e54b4e0b72eb398442841e76d01feb91acc2f3bdf73e2686060b8fc735 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 10 07:18:29 minikube dockerd[812]: time="2024-07-10T07:18:29.832555812Z" level=info msg="ignoring event" container=d5658c8f9a96f66cb4f65bdc8cf44c2326882ae7d9dc0b65f861a355a968ee32 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 10 07:18:30 minikube cri-dockerd[1035]: time="2024-07-10T07:18:30Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f60efe296166e8a5b186dd9612f6a681f2a279a5f5006457caa21715532a4928/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 10 07:18:30 minikube dockerd[812]: time="2024-07-10T07:18:30.170406787Z" level=info msg="ignoring event" container=a1cec64a6252397d3336fd1be420e6a05133706a74e4953adb0c4cb1f7f963b6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 10 07:18:30 minikube cri-dockerd[1035]: time="2024-07-10T07:18:30Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/11b49ebc22072f1e4087a86d0ce3191f3f10ec2bf5eb98e6d4a3ec4e636f8dfc/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 10 07:18:30 minikube cri-dockerd[1035]: time="2024-07-10T07:18:30Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/21ffe311863d2100ee83bf5f48cb9e3a4f41921d5f1916b2184f1bf2cb271cd7/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 10 07:18:35 minikube cri-dockerd[1035]: time="2024-07-10T07:18:35Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Jul 10 07:18:35 minikube dockerd[812]: time="2024-07-10T07:18:35.741324834Z" level=info msg="ignoring event" container=11989ea2df1f9a14497d50e9bb4787104a503f1ce4fff5e10b0afc269f1f79f9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 10 07:18:35 minikube dockerd[812]: time="2024-07-10T07:18:35.977008368Z" level=info msg="ignoring event" container=f60efe296166e8a5b186dd9612f6a681f2a279a5f5006457caa21715532a4928 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 10 07:18:37 minikube cri-dockerd[1035]: time="2024-07-10T07:18:37Z" level=error msg="error getting RW layer size for container ID '6638977e97c35f085810f2a779f327747d6d074a0c1d9d5f692736b1cb5df5a2': Error response from daemon: No such container: 6638977e97c35f085810f2a779f327747d6d074a0c1d9d5f692736b1cb5df5a2"
Jul 10 07:18:37 minikube cri-dockerd[1035]: time="2024-07-10T07:18:37Z" level=error msg="Set backoffDuration to : 1m0s for container ID '6638977e97c35f085810f2a779f327747d6d074a0c1d9d5f692736b1cb5df5a2'"
Jul 10 07:18:37 minikube cri-dockerd[1035]: time="2024-07-10T07:18:37Z" level=error msg="error getting RW layer size for container ID 'b530e8cdc2dc91cd9cc8bf74e70be33239f16fd2567c86dab442357628d1f1a0': Error response from daemon: No such container: b530e8cdc2dc91cd9cc8bf74e70be33239f16fd2567c86dab442357628d1f1a0"
Jul 10 07:18:37 minikube cri-dockerd[1035]: time="2024-07-10T07:18:37Z" level=error msg="Set backoffDuration to : 1m0s for container ID 'b530e8cdc2dc91cd9cc8bf74e70be33239f16fd2567c86dab442357628d1f1a0'"
Jul 10 07:18:38 minikube cri-dockerd[1035]: time="2024-07-10T07:18:38Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Jul 10 07:18:38 minikube dockerd[812]: time="2024-07-10T07:18:38.788662264Z" level=info msg="ignoring event" container=a5c3ebca226c05cbe8f724ddc905488f37b59e8387d4794427a99b61cfb47fb4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 10 07:18:39 minikube dockerd[812]: time="2024-07-10T07:18:39.003579739Z" level=info msg="ignoring event" container=21ffe311863d2100ee83bf5f48cb9e3a4f41921d5f1916b2184f1bf2cb271cd7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 10 07:18:42 minikube cri-dockerd[1035]: time="2024-07-10T07:18:42Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Jul 10 07:18:43 minikube dockerd[812]: time="2024-07-10T07:18:43.897153002Z" level=info msg="ignoring event" container=f1e10230067f1bd66f7aa97f54445a0dddeb9cc53f81299ceb6e4956157c28e4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 10 07:18:44 minikube dockerd[812]: time="2024-07-10T07:18:44.141962496Z" level=info msg="ignoring event" container=11b49ebc22072f1e4087a86d0ce3191f3f10ec2bf5eb98e6d4a3ec4e636f8dfc module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 10 07:21:03 minikube cri-dockerd[1035]: time="2024-07-10T07:21:03Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4c41bc834fd296dfbcc910409e31a6c97ae446f2e7d4b19aab79f58cfd10baae/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 10 07:21:04 minikube cri-dockerd[1035]: time="2024-07-10T07:21:04Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5c3048bf912f6425ca3fa49c4b09628e36ad998fa13b8e14ea88f7cf470f7d75/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 10 07:21:04 minikube cri-dockerd[1035]: time="2024-07-10T07:21:04Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8483ef6296f6eefbd09bc9cf529094296fc724bbeecd03729b565962f69160e6/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 10 07:21:04 minikube cri-dockerd[1035]: time="2024-07-10T07:21:04Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a39eeafe8e35a76da78eb6d77e4612c29700175520a47d9183129342deeb5776/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 10 07:21:08 minikube cri-dockerd[1035]: time="2024-07-10T07:21:08Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Jul 10 07:21:10 minikube cri-dockerd[1035]: time="2024-07-10T07:21:10Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Jul 10 07:21:13 minikube cri-dockerd[1035]: time="2024-07-10T07:21:13Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Jul 10 07:21:16 minikube cri-dockerd[1035]: time="2024-07-10T07:21:16Z" level=info msg="Stop pulling image khannedy/nginx-curl:latest: Status: Image is up to date for khannedy/nginx-curl:latest"
Jul 10 07:40:31 minikube dockerd[812]: time="2024-07-10T07:40:31.170016118Z" level=info msg="ignoring event" container=df7cee9e7ffff8c430b8f5ae304e95303cd7e0bbd5e6196db973d65d68bf0a56 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 10 07:40:31 minikube dockerd[812]: time="2024-07-10T07:40:31.297832093Z" level=info msg="ignoring event" container=30496464776becd30597920fc06dfc1a3f9bf187ab7fe11f254d0947aa60ddac module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 10 07:40:31 minikube dockerd[812]: time="2024-07-10T07:40:31.405066164Z" level=info msg="ignoring event" container=45229c66bb3e5a767721146d733337a387c92972d15b1fef33849c8e669a230e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 10 07:40:31 minikube dockerd[812]: time="2024-07-10T07:40:31.482797562Z" level=info msg="ignoring event" container=93b7dd60f874a60a21cc3d993072305ceb06f12247fbf657aff06049457c940b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 10 07:40:31 minikube dockerd[812]: time="2024-07-10T07:40:31.854045927Z" level=info msg="ignoring event" container=a39eeafe8e35a76da78eb6d77e4612c29700175520a47d9183129342deeb5776 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 10 07:40:32 minikube cri-dockerd[1035]: time="2024-07-10T07:40:32Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"nginx-b2lt2_default\": unexpected command output nsenter: cannot open /proc/28862/ns/net: No such file or directory\n with error: exit status 1"
Jul 10 07:40:32 minikube dockerd[812]: time="2024-07-10T07:40:32.256423644Z" level=info msg="ignoring event" container=5c3048bf912f6425ca3fa49c4b09628e36ad998fa13b8e14ea88f7cf470f7d75 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 10 07:40:32 minikube dockerd[812]: time="2024-07-10T07:40:32.299185922Z" level=info msg="ignoring event" container=8483ef6296f6eefbd09bc9cf529094296fc724bbeecd03729b565962f69160e6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 10 07:40:32 minikube cri-dockerd[1035]: time="2024-07-10T07:40:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0da456dfeebd28b2724aa498e1fce9281e9edaa83d2e20183fd4a7b4c9fa468f/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 10 07:40:32 minikube dockerd[812]: time="2024-07-10T07:40:32.333818397Z" level=info msg="ignoring event" container=4c41bc834fd296dfbcc910409e31a6c97ae446f2e7d4b19aab79f58cfd10baae module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 10 07:40:32 minikube cri-dockerd[1035]: time="2024-07-10T07:40:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e63c54d16eba8b18b018123e9e55d00347b6ada973f217d8b29c5955997310a2/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 10 07:40:35 minikube cri-dockerd[1035]: time="2024-07-10T07:40:35Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Jul 10 07:40:38 minikube cri-dockerd[1035]: time="2024-07-10T07:40:38Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Jul 10 07:40:38 minikube dockerd[812]: time="2024-07-10T07:40:38.990451570Z" level=info msg="ignoring event" container=3998f50ce791bb49ca98b4a97799a43aa7d8d4b0d9a4c61c36e5ade290c0d6c0 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 10 07:40:39 minikube dockerd[812]: time="2024-07-10T07:40:39.286948961Z" level=info msg="ignoring event" container=e63c54d16eba8b18b018123e9e55d00347b6ada973f217d8b29c5955997310a2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 10 07:41:05 minikube dockerd[812]: time="2024-07-10T07:41:05.891855917Z" level=info msg="Container failed to exit within 30s of signal 3 - using the force" container=09d2acbcc87a39dcd30b9a0a81b578b48512eebc0d5a0a0a90341a3bd9d0483d spanID=a37d609ab6edc84b traceID=abfe0879b73688fbb2044a3e19c315be
Jul 10 07:41:05 minikube dockerd[812]: time="2024-07-10T07:41:05.986108481Z" level=info msg="ignoring event" container=09d2acbcc87a39dcd30b9a0a81b578b48512eebc0d5a0a0a90341a3bd9d0483d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 10 07:41:06 minikube cri-dockerd[1035]: time="2024-07-10T07:41:06Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"nginx-ws4xs_default\": unexpected command output nsenter: cannot open /proc/33567/ns/net: No such file or directory\n with error: exit status 1"
Jul 10 07:41:06 minikube dockerd[812]: time="2024-07-10T07:41:06.285742767Z" level=info msg="ignoring event" container=0da456dfeebd28b2724aa498e1fce9281e9edaa83d2e20183fd4a7b4c9fa468f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 10 07:41:19 minikube cri-dockerd[1035]: time="2024-07-10T07:41:19Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2706fb52c5d145fbc16939d3c797af3a1698df9b0a71a7af4bacb8e4c49979ec/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 10 07:41:21 minikube cri-dockerd[1035]: time="2024-07-10T07:41:21Z" level=info msg="Stop pulling image khannedy/nginx-curl:latest: Status: Image is up to date for khannedy/nginx-curl:latest"
Jul 10 07:56:05 minikube dockerd[812]: time="2024-07-10T07:56:05.463053119Z" level=info msg="ignoring event" container=cb233d8b9ab1b26d175e0daea9cf00ab1fcd3ed60cd3b3b356ec14c0c0b4129a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 10 07:56:05 minikube dockerd[812]: time="2024-07-10T07:56:05.719662533Z" level=info msg="ignoring event" container=2706fb52c5d145fbc16939d3c797af3a1698df9b0a71a7af4bacb8e4c49979ec module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 10 07:59:02 minikube cri-dockerd[1035]: time="2024-07-10T07:59:02Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d1538e18d6eb5664a41676951da00f26d3d21d5ee96503dc96c3e6104f6727c9/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 10 07:59:02 minikube cri-dockerd[1035]: time="2024-07-10T07:59:02Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b98ba5628ed359a84286cd80537b38c93d8e62324e0428e16cf0e4261d0ec136/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 10 07:59:02 minikube cri-dockerd[1035]: time="2024-07-10T07:59:02Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e886c27c50005df565dcd0343b7d92a95afe86727fd64d69f54e6965289f0804/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 10 07:59:05 minikube cri-dockerd[1035]: time="2024-07-10T07:59:05Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Jul 10 07:59:08 minikube cri-dockerd[1035]: time="2024-07-10T07:59:08Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Jul 10 07:59:11 minikube cri-dockerd[1035]: time="2024-07-10T07:59:11Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"


==> container status <==
CONTAINER           IMAGE                                                                           CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
a5be9ca097f08       nginx@sha256:67682bda769fae1ccf5183192b8daf37b64cae99c6c3302650f6f8bf5f0f95df   10 minutes ago      Running             nginx                     0                   e886c27c50005       nginx-ncp8b
b2610384db6fe       nginx@sha256:67682bda769fae1ccf5183192b8daf37b64cae99c6c3302650f6f8bf5f0f95df   10 minutes ago      Running             nginx                     0                   b98ba5628ed35       nginx-ghhqn
fe14be52bc430       nginx@sha256:67682bda769fae1ccf5183192b8daf37b64cae99c6c3302650f6f8bf5f0f95df   10 minutes ago      Running             nginx                     0                   d1538e18d6eb5       nginx-48t55
ebbf7ae3be7a6       6e38f40d628db                                                                   3 hours ago         Running             storage-provisioner       32                  7c5f5460bf799       storage-provisioner
b5b6a84535b14       cbb01a7bd410d                                                                   3 hours ago         Running             coredns                   7                   3dfbbe1abc12e       coredns-7db6d8ff4d-v795g
a1930c3168209       6e38f40d628db                                                                   3 hours ago         Exited              storage-provisioner       31                  7c5f5460bf799       storage-provisioner
0ca8230c062c9       a0bf559e280cf                                                                   3 hours ago         Running             kube-proxy                7                   fefceee0b19e8       kube-proxy-7c8fb
51d84d5e4f60e       c7aad43836fa5                                                                   3 hours ago         Running             kube-controller-manager   9                   5e7c7ccdc1b01       kube-controller-manager-minikube
8f00b49876e8c       3861cfcd7c04c                                                                   3 hours ago         Running             etcd                      7                   aef8f65d0a6db       etcd-minikube
15ad977ace556       259c8277fcbbc                                                                   3 hours ago         Running             kube-scheduler            7                   468c2b3f5a988       kube-scheduler-minikube
ed5c8aa6b03db       c42f13656d0b2                                                                   3 hours ago         Running             kube-apiserver            8                   38500746afbdf       kube-apiserver-minikube
75ea7c769f20d       cbb01a7bd410d                                                                   7 hours ago         Exited              coredns                   6                   303dfe4f8379a       coredns-7db6d8ff4d-v795g
322116d6818fc       a0bf559e280cf                                                                   7 hours ago         Exited              kube-proxy                6                   2ad4e0f42cf92       kube-proxy-7c8fb
4dbbd057aeb88       3861cfcd7c04c                                                                   7 hours ago         Exited              etcd                      6                   1224dd81e8234       etcd-minikube
5351dab4d4661       c7aad43836fa5                                                                   7 hours ago         Exited              kube-controller-manager   8                   28cbdaef3eb73       kube-controller-manager-minikube
8ec4a256cecd0       259c8277fcbbc                                                                   7 hours ago         Exited              kube-scheduler            6                   cce9809eff41a       kube-scheduler-minikube
27d47a3b44d55       c42f13656d0b2                                                                   7 hours ago         Exited              kube-apiserver            7                   03eea3fe39fd1       kube-apiserver-minikube


==> coredns [75ea7c769f20] <==
.:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:56420 - 60578 "HINFO IN 8562668107871097980.3514732817862081593. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.039930037s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> coredns [b5b6a84535b1] <==
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:40957 - 50842 "HINFO IN 3176951158789302138.3240290763660599485. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.038778856s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[543291374]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (10-Jul-2024 05:34:27.248) (total time: 30002ms):
Trace[543291374]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30001ms (05:34:57.249)
Trace[543291374]: [30.002505804s] [30.002505804s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[1077833541]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (10-Jul-2024 05:34:27.253) (total time: 30010ms):
Trace[1077833541]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30010ms (05:34:57.264)
Trace[1077833541]: [30.010993667s] [30.010993667s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[838271842]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (10-Jul-2024 05:34:27.266) (total time: 30005ms):
Trace[838271842]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30005ms (05:34:57.272)
Trace[838271842]: [30.005604547s] [30.005604547s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] 10.244.0.63:47765 - 54532 "AAAA IN nginx-service.default.svc.cluster.local.default.svc.cluster.local. udp 83 false 512" NXDOMAIN qr,aa,rd 176 0.000266717s
[INFO] 10.244.0.63:47765 - 54328 "A IN nginx-service.default.svc.cluster.local.default.svc.cluster.local. udp 83 false 512" NXDOMAIN qr,aa,rd 176 0.000295726s
[INFO] 10.244.0.63:51197 - 16796 "AAAA IN nginx-service.default.svc.cluster.local.svc.cluster.local. udp 75 false 512" NXDOMAIN qr,aa,rd 168 0.000508661s
[INFO] 10.244.0.63:51197 - 16610 "A IN nginx-service.default.svc.cluster.local.svc.cluster.local. udp 75 false 512" NXDOMAIN qr,aa,rd 168 0.000365725s
[INFO] 10.244.0.63:54130 - 38105 "AAAA IN nginx-service.default.svc.cluster.local.cluster.local. udp 71 false 512" NXDOMAIN qr,aa,rd 164 0.000043316s
[INFO] 10.244.0.63:54130 - 37960 "A IN nginx-service.default.svc.cluster.local.cluster.local. udp 71 false 512" NXDOMAIN qr,aa,rd 164 0.000177609s
[INFO] 10.244.0.63:40599 - 47530 "AAAA IN nginx-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 150 0.000053335s
[INFO] 10.244.0.63:40599 - 47394 "A IN nginx-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 112 0.00006901s
[INFO] 10.244.0.66:41582 - 21637 "AAAA IN example-service.default.svc.cluster.local.default.svc.cluster.local. udp 85 false 512" NXDOMAIN qr,aa,rd 178 0.000186272s
[INFO] 10.244.0.66:41582 - 21412 "A IN example-service.default.svc.cluster.local.default.svc.cluster.local. udp 85 false 512" NXDOMAIN qr,aa,rd 178 0.000887557s
[INFO] 10.244.0.66:45337 - 22826 "AAAA IN example-service.default.svc.cluster.local.svc.cluster.local. udp 77 false 512" NXDOMAIN qr,aa,rd 170 0.00118474s
[INFO] 10.244.0.66:45337 - 22648 "A IN example-service.default.svc.cluster.local.svc.cluster.local. udp 77 false 512" NXDOMAIN qr,aa,rd 170 0.001239915s
[INFO] 10.244.0.66:53357 - 16271 "AAAA IN example-service.default.svc.cluster.local.cluster.local. udp 73 false 512" NXDOMAIN qr,aa,rd 166 0.000052543s
[INFO] 10.244.0.66:53357 - 16050 "A IN example-service.default.svc.cluster.local.cluster.local. udp 73 false 512" NXDOMAIN qr,aa,rd 166 0.000427992s
[INFO] 10.244.0.66:50431 - 16826 "A IN example.com. udp 29 false 512" NOERROR qr,rd,ra 302 0.014424615s
[INFO] 10.244.0.66:50431 - 16826 "A IN example-service.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 152 0.014628308s
[INFO] 10.244.0.66:50431 - 17006 "AAAA IN example.com. udp 29 false 512" NOERROR qr,rd,ra 314 0.305226032s
[INFO] 10.244.0.66:50431 - 17006 "AAAA IN example-service.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 164 0.305921348s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=5883c09216182566a63dff4c326a6fc9ed2982ff
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_07_07T13_41_39_0700
                    minikube.k8s.io/version=v1.33.1
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sun, 07 Jul 2024 06:41:35 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Wed, 10 Jul 2024 08:09:38 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Wed, 10 Jul 2024 08:04:57 +0000   Mon, 08 Jul 2024 09:09:35 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 10 Jul 2024 08:04:57 +0000   Mon, 08 Jul 2024 09:09:35 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 10 Jul 2024 08:04:57 +0000   Mon, 08 Jul 2024 09:09:35 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 10 Jul 2024 08:04:57 +0000   Tue, 09 Jul 2024 07:55:57 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                2
  ephemeral-storage:  35292736Ki
  hugepages-2Mi:      0
  memory:             4001012Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  35292736Ki
  hugepages-2Mi:      0
  memory:             4001012Ki
  pods:               110
System Info:
  Machine ID:                 a21135892b154383bae5f91d022072d2
  System UUID:                8f68dd9c-4d26-4c86-8ee2-1afd33f7205a
  Boot ID:                    30eeb0a8-f70d-4d42-bc2d-19cf52d01938
  Kernel Version:             6.5.0-41-generic
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://26.1.1
  Kubelet Version:            v1.30.0
  Kube-Proxy Version:         v1.30.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (10 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     nginx-48t55                         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10m
  default                     nginx-ghhqn                         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10m
  default                     nginx-ncp8b                         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10m
  kube-system                 coredns-7db6d8ff4d-v795g            100m (5%!)(MISSING)     0 (0%!)(MISSING)      70Mi (1%!)(MISSING)        170Mi (4%!)(MISSING)     3d1h
  kube-system                 etcd-minikube                       100m (5%!)(MISSING)     0 (0%!)(MISSING)      100Mi (2%!)(MISSING)       0 (0%!)(MISSING)         3d1h
  kube-system                 kube-apiserver-minikube             250m (12%!)(MISSING)    0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d1h
  kube-system                 kube-controller-manager-minikube    200m (10%!)(MISSING)    0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d1h
  kube-system                 kube-proxy-7c8fb                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d1h
  kube-system                 kube-scheduler-minikube             100m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d1h
  kube-system                 storage-provisioner                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d1h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (37%!)(MISSING)  0 (0%!)(MISSING)
  memory             170Mi (4%!)(MISSING)  170Mi (4%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:              <none>


==> dmesg <==
[Jul10 04:52] APIC calibration not consistent with PM-Timer: 96ms instead of 100ms
[  +0.006914] TSC synchronization [CPU#0 -> CPU#1]:
[  +0.000003] Measured 46120 cycles TSC warp between CPUs, turning off TSC clock.
[  +0.051506] acpi PNP0A03:00: fail to add MMCONFIG information, can't access extended configuration space under this bridge
[  +0.344577] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +0.000076] platform eisa.0: EISA: Cannot allocate resource for mainboard
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 1
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 2
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 3
[  +0.000000] platform eisa.0: Cannot allocate resource for EISA slot 4
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 5
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 6
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 7
[  +0.000000] platform eisa.0: Cannot allocate resource for EISA slot 8
[  +1.928588] systemd[1]: memfd_create() called without MFD_EXEC or MFD_NOEXEC_SEAL set
[  +0.089702] block sda: the capability attribute has been deprecated.
[  +0.089191] systemd[1]: Configuration file /run/systemd/system/netplan-ovs-cleanup.service is marked world-inaccessible. This has no effect as configuration data is accessible via APIs without restrictions. Proceeding anyway.
[  +1.158206] vmwgfx 0000:00:02.0: [drm] *ERROR* vmwgfx seems to be running on an unsupported hypervisor.
[  +0.000004] vmwgfx 0000:00:02.0: [drm] *ERROR* This configuration is likely broken.
[  +0.000001] vmwgfx 0000:00:02.0: [drm] *ERROR* Please switch to a supported graphics device to avoid problems.
[  +0.424238] workqueue: drm_fb_helper_damage_work [drm_kms_helper] hogged CPU for >10000us 4 times, consider switching to WQ_UNBOUND
[  +1.825893] kauditd_printk_skb: 38 callbacks suppressed
[Jul10 05:36] workqueue: blk_mq_run_work_fn hogged CPU for >10000us 4 times, consider switching to WQ_UNBOUND
[Jul10 06:48] workqueue: blk_mq_run_work_fn hogged CPU for >10000us 8 times, consider switching to WQ_UNBOUND


==> etcd [4dbbd057aeb8] <==
{"level":"info","ts":"2024-07-10T03:34:23.066762Z","caller":"etcdserver/server.go:1401","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":60006,"local-member-snapshot-index":50005,"local-member-snapshot-count":10000}
{"level":"info","ts":"2024-07-10T03:34:23.078096Z","caller":"etcdserver/server.go:2420","msg":"saved snapshot","snapshot-index":60006}
{"level":"info","ts":"2024-07-10T03:34:23.079258Z","caller":"etcdserver/server.go:2450","msg":"compacted Raft logs","compact-index":55006}
{"level":"info","ts":"2024-07-10T03:34:35.785541Z","caller":"fileutil/purge.go:96","msg":"purged","path":"/var/lib/minikube/etcd/member/snap/0000000000000004-0000000000002711.snap"}
{"level":"info","ts":"2024-07-10T03:38:06.368591Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":48337}
{"level":"info","ts":"2024-07-10T03:38:06.377414Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":48337,"took":"8.45672ms","hash":2857195026,"current-db-size-bytes":2502656,"current-db-size":"2.5 MB","current-db-size-in-use-bytes":1392640,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-07-10T03:38:06.377514Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2857195026,"revision":48337,"compact-revision":48094}
{"level":"info","ts":"2024-07-10T03:43:06.392151Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":48577}
{"level":"info","ts":"2024-07-10T03:43:06.551626Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":48577,"took":"158.519126ms","hash":3995239721,"current-db-size-bytes":2502656,"current-db-size":"2.5 MB","current-db-size-in-use-bytes":1413120,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-07-10T03:43:06.551663Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3995239721,"revision":48577,"compact-revision":48337}
{"level":"info","ts":"2024-07-10T03:48:06.408318Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":48823}
{"level":"info","ts":"2024-07-10T03:48:06.463835Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":48823,"took":"54.297901ms","hash":1024843123,"current-db-size-bytes":2502656,"current-db-size":"2.5 MB","current-db-size-in-use-bytes":1404928,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-07-10T03:48:06.463928Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1024843123,"revision":48823,"compact-revision":48577}
{"level":"info","ts":"2024-07-10T03:53:06.426316Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":49076}
{"level":"info","ts":"2024-07-10T03:53:06.433649Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":49076,"took":"7.123057ms","hash":4236956541,"current-db-size-bytes":2502656,"current-db-size":"2.5 MB","current-db-size-in-use-bytes":1400832,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-07-10T03:53:06.433736Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4236956541,"revision":49076,"compact-revision":48823}
{"level":"info","ts":"2024-07-10T03:58:06.448414Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":49318}
{"level":"info","ts":"2024-07-10T03:58:06.454408Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":49318,"took":"5.716036ms","hash":3190657176,"current-db-size-bytes":2502656,"current-db-size":"2.5 MB","current-db-size-in-use-bytes":1437696,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-07-10T03:58:06.454674Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3190657176,"revision":49318,"compact-revision":49076}
{"level":"info","ts":"2024-07-10T04:03:06.468927Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":49588}
{"level":"info","ts":"2024-07-10T04:03:06.476262Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":49588,"took":"7.139098ms","hash":430216538,"current-db-size-bytes":2502656,"current-db-size":"2.5 MB","current-db-size-in-use-bytes":1466368,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2024-07-10T04:03:06.476348Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":430216538,"revision":49588,"compact-revision":49318}
{"level":"info","ts":"2024-07-10T04:08:06.506881Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":49831}
{"level":"info","ts":"2024-07-10T04:08:06.516975Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":49831,"took":"9.545203ms","hash":1839695169,"current-db-size-bytes":2502656,"current-db-size":"2.5 MB","current-db-size-in-use-bytes":1359872,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-07-10T04:08:06.517547Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1839695169,"revision":49831,"compact-revision":49588}
{"level":"info","ts":"2024-07-10T04:13:06.529257Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":50070}
{"level":"info","ts":"2024-07-10T04:13:06.534807Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":50070,"took":"5.249423ms","hash":925379233,"current-db-size-bytes":2502656,"current-db-size":"2.5 MB","current-db-size-in-use-bytes":1400832,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-07-10T04:13:06.535041Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":925379233,"revision":50070,"compact-revision":49831}
{"level":"info","ts":"2024-07-10T04:18:06.564394Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":50349}
{"level":"info","ts":"2024-07-10T04:18:06.573475Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":50349,"took":"8.883956ms","hash":3547584157,"current-db-size-bytes":2502656,"current-db-size":"2.5 MB","current-db-size-in-use-bytes":1503232,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2024-07-10T04:18:06.573685Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3547584157,"revision":50349,"compact-revision":50070}
{"level":"info","ts":"2024-07-10T04:23:06.586941Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":50610}
{"level":"info","ts":"2024-07-10T04:23:06.595787Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":50610,"took":"8.67477ms","hash":1101972901,"current-db-size-bytes":2502656,"current-db-size":"2.5 MB","current-db-size-in-use-bytes":1536000,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2024-07-10T04:23:06.596191Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1101972901,"revision":50610,"compact-revision":50349}
{"level":"info","ts":"2024-07-10T04:28:06.618678Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":50874}
{"level":"info","ts":"2024-07-10T04:28:06.631547Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":50874,"took":"12.676888ms","hash":3917679071,"current-db-size-bytes":2502656,"current-db-size":"2.5 MB","current-db-size-in-use-bytes":1462272,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2024-07-10T04:28:06.631585Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3917679071,"revision":50874,"compact-revision":50610}
{"level":"info","ts":"2024-07-10T04:31:58.637902Z","caller":"wal/wal.go:785","msg":"created a new WAL segment","path":"/var/lib/minikube/etcd/member/wal/0000000000000001-000000000000f899.wal"}
{"level":"info","ts":"2024-07-10T04:33:06.643863Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":51118}
{"level":"info","ts":"2024-07-10T04:33:06.652356Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":51118,"took":"6.76131ms","hash":119143273,"current-db-size-bytes":2502656,"current-db-size":"2.5 MB","current-db-size-in-use-bytes":1376256,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-07-10T04:33:06.652497Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":119143273,"revision":51118,"compact-revision":50874}
{"level":"info","ts":"2024-07-10T04:38:06.662751Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":51357}
{"level":"info","ts":"2024-07-10T04:38:06.667872Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":51357,"took":"4.711185ms","hash":2514750038,"current-db-size-bytes":2502656,"current-db-size":"2.5 MB","current-db-size-in-use-bytes":1396736,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-07-10T04:38:06.668038Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2514750038,"revision":51357,"compact-revision":51118}
{"level":"info","ts":"2024-07-10T04:43:06.704647Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":51594}
{"level":"info","ts":"2024-07-10T04:43:06.71291Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":51594,"took":"7.336912ms","hash":928829138,"current-db-size-bytes":2502656,"current-db-size":"2.5 MB","current-db-size-in-use-bytes":1413120,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-07-10T04:43:06.71315Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":928829138,"revision":51594,"compact-revision":51357}
{"level":"info","ts":"2024-07-10T04:48:06.729812Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":51851}
{"level":"info","ts":"2024-07-10T04:48:06.736558Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":51851,"took":"6.02121ms","hash":1665487436,"current-db-size-bytes":2502656,"current-db-size":"2.5 MB","current-db-size-in-use-bytes":1417216,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-07-10T04:48:06.736901Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1665487436,"revision":51851,"compact-revision":51594}
{"level":"info","ts":"2024-07-10T04:50:32.965522Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2024-07-10T04:50:32.977494Z","caller":"embed/etcd.go:375","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2024-07-10T04:50:32.985711Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-07-10T04:50:32.993546Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-07-10T04:50:32.993653Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2024-07-10T04:50:32.993663Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2024-07-10T04:50:33.00103Z","caller":"etcdserver/server.go:1471","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-07-10T04:50:33.018454Z","caller":"embed/etcd.go:579","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-07-10T04:50:33.020001Z","caller":"embed/etcd.go:584","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-07-10T04:50:33.020032Z","caller":"embed/etcd.go:377","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> etcd [8f00b49876e8] <==
{"level":"info","ts":"2024-07-10T06:39:21.847327Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":55405,"took":"7.157837ms","hash":2726487151,"current-db-size-bytes":2502656,"current-db-size":"2.5 MB","current-db-size-in-use-bytes":1892352,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2024-07-10T06:39:21.847387Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2726487151,"revision":55405,"compact-revision":55034}
{"level":"info","ts":"2024-07-10T06:44:21.853043Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":55662}
{"level":"info","ts":"2024-07-10T06:44:21.860172Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":55662,"took":"6.968376ms","hash":357360491,"current-db-size-bytes":2502656,"current-db-size":"2.5 MB","current-db-size-in-use-bytes":1613824,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-07-10T06:44:21.860215Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":357360491,"revision":55662,"compact-revision":55405}
{"level":"info","ts":"2024-07-10T06:49:21.871546Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":55902}
{"level":"info","ts":"2024-07-10T06:49:21.878354Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":55902,"took":"6.188877ms","hash":1465616039,"current-db-size-bytes":2502656,"current-db-size":"2.5 MB","current-db-size-in-use-bytes":1646592,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-07-10T06:49:21.878424Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1465616039,"revision":55902,"compact-revision":55662}
{"level":"info","ts":"2024-07-10T06:53:17.36744Z","caller":"etcdserver/server.go:1401","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":70007,"local-member-snapshot-index":60006,"local-member-snapshot-count":10000}
{"level":"info","ts":"2024-07-10T06:53:17.383266Z","caller":"etcdserver/server.go:2420","msg":"saved snapshot","snapshot-index":70007}
{"level":"info","ts":"2024-07-10T06:53:17.383928Z","caller":"etcdserver/server.go:2450","msg":"compacted Raft logs","compact-index":65007}
{"level":"info","ts":"2024-07-10T06:53:21.083568Z","caller":"fileutil/purge.go:96","msg":"purged","path":"/var/lib/minikube/etcd/member/snap/0000000000000006-0000000000004e22.snap"}
{"level":"info","ts":"2024-07-10T06:54:21.891675Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":56249}
{"level":"info","ts":"2024-07-10T06:54:21.899043Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":56249,"took":"6.901387ms","hash":1051651867,"current-db-size-bytes":2502656,"current-db-size":"2.5 MB","current-db-size-in-use-bytes":1720320,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2024-07-10T06:54:21.899071Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1051651867,"revision":56249,"compact-revision":55902}
{"level":"info","ts":"2024-07-10T06:59:21.911148Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":56487}
{"level":"info","ts":"2024-07-10T06:59:21.916988Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":56487,"took":"5.412593ms","hash":495938272,"current-db-size-bytes":2502656,"current-db-size":"2.5 MB","current-db-size-in-use-bytes":1478656,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2024-07-10T06:59:21.917056Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":495938272,"revision":56487,"compact-revision":56249}
{"level":"info","ts":"2024-07-10T07:04:21.939856Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":56726}
{"level":"info","ts":"2024-07-10T07:04:21.947342Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":56726,"took":"7.233195ms","hash":974710926,"current-db-size-bytes":2502656,"current-db-size":"2.5 MB","current-db-size-in-use-bytes":1478656,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2024-07-10T07:04:21.94806Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":974710926,"revision":56726,"compact-revision":56487}
{"level":"info","ts":"2024-07-10T07:09:21.959271Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":56965}
{"level":"info","ts":"2024-07-10T07:09:21.96724Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":56965,"took":"7.14735ms","hash":1460074841,"current-db-size-bytes":2588672,"current-db-size":"2.6 MB","current-db-size-in-use-bytes":1638400,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-07-10T07:09:21.967464Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1460074841,"revision":56965,"compact-revision":56726}
{"level":"info","ts":"2024-07-10T07:14:21.980597Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":57262}
{"level":"info","ts":"2024-07-10T07:14:21.994558Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":57262,"took":"13.487167ms","hash":2909841534,"current-db-size-bytes":2674688,"current-db-size":"2.7 MB","current-db-size-in-use-bytes":1773568,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2024-07-10T07:14:21.994624Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2909841534,"revision":57262,"compact-revision":56965}
{"level":"info","ts":"2024-07-10T07:19:21.99958Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":57504}
{"level":"info","ts":"2024-07-10T07:19:22.012977Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":57504,"took":"12.961348ms","hash":2924393177,"current-db-size-bytes":2674688,"current-db-size":"2.7 MB","current-db-size-in-use-bytes":1912832,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2024-07-10T07:19:22.013062Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2924393177,"revision":57504,"compact-revision":57262}
{"level":"info","ts":"2024-07-10T07:24:22.015174Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":57835}
{"level":"info","ts":"2024-07-10T07:24:22.030175Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":57835,"took":"14.431117ms","hash":43618963,"current-db-size-bytes":2768896,"current-db-size":"2.8 MB","current-db-size-in-use-bytes":2125824,"current-db-size-in-use":"2.1 MB"}
{"level":"info","ts":"2024-07-10T07:24:22.030471Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":43618963,"revision":57835,"compact-revision":57504}
{"level":"info","ts":"2024-07-10T07:29:22.038798Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":58129}
{"level":"info","ts":"2024-07-10T07:29:22.05399Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":58129,"took":"14.335042ms","hash":2107884374,"current-db-size-bytes":2768896,"current-db-size":"2.8 MB","current-db-size-in-use-bytes":1847296,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2024-07-10T07:29:22.054061Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2107884374,"revision":58129,"compact-revision":57835}
{"level":"info","ts":"2024-07-10T07:34:22.054179Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":58369}
{"level":"info","ts":"2024-07-10T07:34:22.060412Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":58369,"took":"5.853401ms","hash":1430513370,"current-db-size-bytes":2768896,"current-db-size":"2.8 MB","current-db-size-in-use-bytes":1703936,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2024-07-10T07:34:22.060474Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1430513370,"revision":58369,"compact-revision":58129}
{"level":"info","ts":"2024-07-10T07:39:22.074789Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":58613}
{"level":"info","ts":"2024-07-10T07:39:22.08132Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":58613,"took":"5.884398ms","hash":2568833085,"current-db-size-bytes":2768896,"current-db-size":"2.8 MB","current-db-size-in-use-bytes":1724416,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2024-07-10T07:39:22.081705Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2568833085,"revision":58613,"compact-revision":58369}
{"level":"info","ts":"2024-07-10T07:44:22.096544Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":58852}
{"level":"info","ts":"2024-07-10T07:44:22.102783Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":58852,"took":"5.569311ms","hash":52879956,"current-db-size-bytes":2859008,"current-db-size":"2.9 MB","current-db-size-in-use-bytes":1929216,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2024-07-10T07:44:22.102904Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":52879956,"revision":58852,"compact-revision":58613}
{"level":"info","ts":"2024-07-10T07:49:22.125623Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":59185}
{"level":"info","ts":"2024-07-10T07:49:22.139083Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":59185,"took":"13.094011ms","hash":2362844070,"current-db-size-bytes":2859008,"current-db-size":"2.9 MB","current-db-size-in-use-bytes":1941504,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2024-07-10T07:49:22.13978Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2362844070,"revision":59185,"compact-revision":58852}
{"level":"info","ts":"2024-07-10T07:54:22.143015Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":59431}
{"level":"info","ts":"2024-07-10T07:54:22.149422Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":59431,"took":"5.616836ms","hash":2296436963,"current-db-size-bytes":2859008,"current-db-size":"2.9 MB","current-db-size-in-use-bytes":1556480,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-07-10T07:54:22.149492Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2296436963,"revision":59431,"compact-revision":59185}
{"level":"info","ts":"2024-07-10T07:59:22.161986Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":59671}
{"level":"info","ts":"2024-07-10T07:59:22.167862Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":59671,"took":"5.044197ms","hash":2871891447,"current-db-size-bytes":2859008,"current-db-size":"2.9 MB","current-db-size-in-use-bytes":1658880,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2024-07-10T07:59:22.167966Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2871891447,"revision":59671,"compact-revision":59431}
{"level":"info","ts":"2024-07-10T08:04:22.179225Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":59970}
{"level":"info","ts":"2024-07-10T08:04:22.201106Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":59970,"took":"21.195491ms","hash":1941187720,"current-db-size-bytes":2859008,"current-db-size":"2.9 MB","current-db-size-in-use-bytes":1761280,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2024-07-10T08:04:22.201297Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1941187720,"revision":59970,"compact-revision":59671}
{"level":"info","ts":"2024-07-10T08:09:22.197304Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":60209}
{"level":"info","ts":"2024-07-10T08:09:22.203459Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":60209,"took":"5.896925ms","hash":1774391881,"current-db-size-bytes":2859008,"current-db-size":"2.9 MB","current-db-size-in-use-bytes":1597440,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-07-10T08:09:22.20352Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1774391881,"revision":60209,"compact-revision":59970}


==> kernel <==
 08:09:41 up  3:17,  0 users,  load average: 0.38, 0.59, 0.68
Linux minikube 6.5.0-41-generic #41~22.04.2-Ubuntu SMP PREEMPT_DYNAMIC Mon Jun  3 11:32:55 UTC 2 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [27d47a3b44d5] <==
E0710 03:33:53.197526       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2024-07-10 03:33:53.197466049" prevR="2.74301180ss" incrR="184467440737.09548469ss" currentR="2.74298033ss"
E0710 03:40:23.332742       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2024-07-10 03:40:23.332676959" prevR="2.82964692ss" incrR="184467440737.09548141ss" currentR="2.82961217ss"
I0710 03:41:21.653213       1 controller.go:615] quota admission added evaluator for: namespaces
I0710 03:41:21.710090       1 controller.go:615] quota admission added evaluator for: serviceaccounts
E0710 03:44:23.398172       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2024-07-10 03:44:23.398131848" prevR="2.88689287ss" incrR="184467440737.09550225ss" currentR="2.88687896ss"
E0710 03:47:53.541680       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2024-07-10 03:47:53.541663760" prevR="2.95595800ss" incrR="184467440737.09551408ss" currentR="2.95595592ss"
E0710 03:50:23.581380       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2024-07-10 03:50:23.581345243" prevR="2.98152216ss" incrR="184467440737.09548019ss" currentR="2.98148619ss"
E0710 03:54:53.668646       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2024-07-10 03:54:53.668578255" prevR="3.48252199ss" incrR="184467440737.09549282ss" currentR="3.48249865ss"
I0710 04:04:32.153941       1 trace.go:236] Trace[1432524636]: "Get" accept:application/json, */*,audit-id:cabf940e-1159-4dab-9c91-f532e186364d,client:192.168.49.2,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:GET (10-Jul-2024 04:04:31.536) (total time: 607ms):
Trace[1432524636]: ---"About to write a response" 607ms (04:04:32.143)
Trace[1432524636]: [607.692734ms] [607.692734ms] END
E0710 04:06:53.942057       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2024-07-10 04:06:53.942001254" prevR="3.65026916ss" incrR="184467440737.09545786ss" currentR="3.65021086ss"
E0710 04:10:54.029008       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2024-07-10 04:10:54.028981868" prevR="3.71176298ss" incrR="184467440737.09550488ss" currentR="3.71175170ss"
I0710 04:11:53.663603       1 controller.go:615] quota admission added evaluator for: replicasets.apps
E0710 04:11:53.754253       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2024-07-10 04:11:53.754238923" prevR="3.77681930ss" incrR="184467440737.09548216ss" currentR="3.77678530ss"
E0710 04:17:51.236172       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2024-07-10 04:17:51.236154930" prevR="3.95494539ss" incrR="184467440737.09547009ss" currentR="3.95489932ss"
E0710 04:28:54.376605       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2024-07-10 04:28:54.376547150" prevR="4.19213279ss" incrR="184467440737.09547889ss" currentR="4.19209552ss"
E0710 04:36:54.581875       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2024-07-10 04:36:54.581831215" prevR="4.29050943ss" incrR="184467440737.09549105ss" currentR="4.29048432ss"
E0710 04:50:24.800632       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2024-07-10 04:50:24.800602738" prevR="4.45947219ss" incrR="184467440737.09551464ss" currentR="4.45947067ss"
I0710 04:50:30.732876       1 trace.go:236] Trace[1320208024]: "Update" accept:application/json, */*,audit-id:34c6bc39-fb5c-42c5-8b9d-b72f9609a228,client:192.168.49.2,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (10-Jul-2024 04:50:29.909) (total time: 822ms):
Trace[1320208024]: ---"Writing http response done" 370ms (04:50:30.732)
Trace[1320208024]: [822.955301ms] [822.955301ms] END
I0710 04:50:31.833026       1 controller.go:128] Shutting down kubernetes service endpoint reconciler
W0710 04:50:32.284601       1 lease.go:265] Resetting endpoints for master service "kubernetes" to []
I0710 04:50:32.467445       1 controller.go:86] Shutting down OpenAPI V3 AggregationController
I0710 04:50:32.472439       1 storage_flowcontrol.go:187] APF bootstrap ensurer is exiting
I0710 04:50:32.471595       1 object_count_tracker.go:151] "StorageObjectCountTracker pruner is exiting"
I0710 04:50:32.472661       1 autoregister_controller.go:165] Shutting down autoregister controller
I0710 04:50:32.473370       1 cluster_authentication_trust_controller.go:463] Shutting down cluster_authentication_trust_controller controller
I0710 04:50:32.475881       1 controller.go:157] Shutting down quota evaluator
I0710 04:50:32.475893       1 controller.go:176] quota evaluator worker shutdown
I0710 04:50:32.496486       1 controller.go:176] quota evaluator worker shutdown
I0710 04:50:32.496562       1 controller.go:176] quota evaluator worker shutdown
I0710 04:50:32.496567       1 controller.go:176] quota evaluator worker shutdown
I0710 04:50:32.496570       1 controller.go:176] quota evaluator worker shutdown
I0710 04:50:32.498316       1 apiapproval_controller.go:198] Shutting down KubernetesAPIApprovalPolicyConformantConditionController
I0710 04:50:32.516920       1 dynamic_serving_content.go:146] "Shutting down controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0710 04:50:32.517911       1 nonstructuralschema_controller.go:204] Shutting down NonStructuralSchemaConditionController
I0710 04:50:32.518159       1 establishing_controller.go:87] Shutting down EstablishingController
I0710 04:50:32.525387       1 naming_controller.go:302] Shutting down NamingConditionController
I0710 04:50:32.533240       1 crdregistration_controller.go:142] Shutting down crd-autoregister controller
I0710 04:50:32.539068       1 dynamic_cafile_content.go:171] "Shutting down controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0710 04:50:32.539188       1 dynamic_cafile_content.go:171] "Shutting down controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0710 04:50:32.543465       1 controller.go:84] Shutting down OpenAPI AggregationController
I0710 04:50:32.552155       1 controller.go:167] Shutting down OpenAPI controller
I0710 04:50:32.552503       1 system_namespaces_controller.go:77] Shutting down system namespaces controller
I0710 04:50:32.564444       1 controller.go:129] Ending legacy_token_tracking_controller
I0710 04:50:32.564542       1 controller.go:130] Shutting down legacy_token_tracking_controller
I0710 04:50:32.571566       1 apf_controller.go:386] Shutting down API Priority and Fairness config worker
I0710 04:50:32.574785       1 available_controller.go:439] Shutting down AvailableConditionController
I0710 04:50:32.574836       1 gc_controller.go:91] Shutting down apiserver lease garbage collector
I0710 04:50:32.574863       1 customresource_discovery_controller.go:325] Shutting down DiscoveryController
I0710 04:50:32.574879       1 crd_finalizer.go:278] Shutting down CRDFinalizer
I0710 04:50:32.574895       1 apiservice_controller.go:131] Shutting down APIServiceRegistrationController
I0710 04:50:32.576584       1 secure_serving.go:258] Stopped listening on [::]:8443
I0710 04:50:32.576604       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
I0710 04:50:32.576624       1 dynamic_cafile_content.go:171] "Shutting down controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0710 04:50:32.580588       1 controller.go:117] Shutting down OpenAPI V3 controller
I0710 04:50:32.599427       1 dynamic_serving_content.go:146] "Shutting down controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0710 04:50:32.599644       1 dynamic_cafile_content.go:171] "Shutting down controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"


==> kube-apiserver [ed5c8aa6b03d] <==
I0710 05:34:23.205192       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0710 05:34:23.205218       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0710 05:34:23.205241       1 apf_controller.go:374] Starting API Priority and Fairness config controller
I0710 05:34:23.210479       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0710 05:34:23.210583       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0710 05:34:23.249005       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0710 05:34:23.249351       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0710 05:34:23.250248       1 controller.go:139] Starting OpenAPI controller
I0710 05:34:23.250482       1 controller.go:87] Starting OpenAPI V3 controller
I0710 05:34:23.250686       1 naming_controller.go:291] Starting NamingConditionController
I0710 05:34:23.250886       1 establishing_controller.go:76] Starting EstablishingController
I0710 05:34:23.251094       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0710 05:34:23.251305       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0710 05:34:23.251404       1 crd_finalizer.go:266] Starting CRDFinalizer
I0710 05:34:23.393800       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0710 05:34:23.397926       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0710 05:34:23.408260       1 apf_controller.go:379] Running API Priority and Fairness config worker
I0710 05:34:23.408280       1 apf_controller.go:382] Running API Priority and Fairness periodic rebalancing process
I0710 05:34:23.410662       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0710 05:34:23.411225       1 shared_informer.go:320] Caches are synced for configmaps
I0710 05:34:23.411804       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0710 05:34:23.416871       1 aggregator.go:165] initial CRD sync complete...
I0710 05:34:23.418873       1 autoregister_controller.go:141] Starting autoregister controller
I0710 05:34:23.419188       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0710 05:34:23.419197       1 cache.go:39] Caches are synced for autoregister controller
I0710 05:34:23.419364       1 handler_discovery.go:447] Starting ResourceDiscoveryManager
I0710 05:34:23.436201       1 shared_informer.go:320] Caches are synced for node_authorizer
E0710 05:34:23.442310       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2024-07-10 05:34:23.442288656" prevR="0.00117732ss" incrR="184467440737.09551401ss" currentR="0.00117517ss"
I0710 05:34:23.476751       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0710 05:34:23.476830       1 policy_source.go:224] refreshing policies
I0710 05:34:23.491204       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
E0710 05:34:23.524388       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2024-07-10 05:34:23.524376922" prevR="0.00655361ss" incrR="184467440737.09550967ss" currentR="0.00654712ss"
E0710 05:34:23.524907       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2024-07-10 05:34:23.524900395" prevR="0.00695823ss" incrR="184467440737.09550844ss" currentR="0.00695051ss"
I0710 05:34:24.228158       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
W0710 05:34:24.943426       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0710 05:34:24.948143       1 controller.go:615] quota admission added evaluator for: endpoints
I0710 05:34:24.972920       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0710 05:34:56.814743       1 controller.go:615] quota admission added evaluator for: replicasets.apps
E0710 05:35:25.763687       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2024-07-10 05:35:25.763672349" prevR="0.48193184ss" incrR="184467440737.09551001ss" currentR="0.48192569ss"
E0710 05:49:27.692371       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2024-07-10 05:49:27.692297302" prevR="0.62386992ss" incrR="184467440737.09551422ss" currentR="0.62386798ss"
I0710 06:07:13.263824       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I0710 06:07:13.291920       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps
E0710 06:10:07.698575       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2024-07-10 06:10:07.698561396" prevR="0.84838816ss" incrR="184467440737.09550307ss" currentR="0.84837507ss"
I0710 06:29:00.775222       1 controller.go:615] quota admission added evaluator for: jobs.batch
E0710 06:36:56.068943       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2024-07-10 06:36:56.068917048" prevR="1.38560255ss" incrR="184467440737.09550956ss" currentR="1.38559595ss"
I0710 06:44:47.833974       1 controller.go:615] quota admission added evaluator for: cronjobs.batch
I0710 07:08:15.779708       1 alloc.go:330] "allocated clusterIPs" service="default/nginx-service" clusterIPs={"IPv4":"10.105.246.95"}
E0710 07:18:18.868274       1 conn.go:339] Error on socket receive: read tcp 192.168.49.2:8443->192.168.49.1:57154: use of closed network connection
I0710 07:18:37.965241       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0710 07:18:37.977292       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0710 07:21:02.989410       1 alloc.go:330] "allocated clusterIPs" service="default/nginx-service" clusterIPs={"IPv4":"10.106.194.79"}
E0710 07:29:04.435386       1 conn.go:339] Error on socket receive: read tcp 192.168.49.2:8443->192.168.49.1:46188: use of closed network connection
I0710 07:40:38.072105       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0710 07:40:38.087654       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
E0710 07:47:54.661208       1 conn.go:339] Error on socket receive: read tcp 192.168.49.2:8443->192.168.49.1:54142: use of closed network connection
I0710 07:56:08.363557       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0710 07:56:08.374811       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
E0710 07:59:01.018183       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2024-07-10 07:59:01.018157440" prevR="4.26373989ss" incrR="184467440737.09550788ss" currentR="4.26373161ss"
I0710 07:59:01.156918       1 alloc.go:330] "allocated clusterIPs" service="default/nginx-service" clusterIPs={"IPv4":"10.100.103.0"}
E0710 08:00:25.300734       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2024-07-10 08:00:25.300720966" prevR="4.43569490ss" incrR="184467440737.09550802ss" currentR="4.43568676ss"


==> kube-controller-manager [51d84d5e4f60] <==
I0710 06:48:00.140624       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="default/nodejs-cronjob-28676568"
I0710 06:48:00.151052       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="default/nodejs-cronjob-28676568"
I0710 06:48:00.169749       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="default/nodejs-cronjob-28676568"
I0710 06:48:03.945532       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="default/nodejs-cronjob-28676568"
I0710 06:48:05.273669       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="default/nodejs-cronjob-28676568"
I0710 06:48:06.011138       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="default/nodejs-cronjob-28676568"
I0710 06:48:06.285378       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="default/nodejs-cronjob-28676568"
I0710 06:48:06.299767       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="default/nodejs-cronjob-28676568"
I0710 06:48:06.306981       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="default/nodejs-cronjob-28676568"
I0710 06:48:06.317305       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="default/nodejs-cronjob-28676565"
E0710 06:48:06.329231       1 cronjob_controllerv2.go:168] error syncing CronJobController default/nodejs-cronjob, requeuing: Operation cannot be fulfilled on cronjobs.batch "nodejs-cronjob": the object has been modified; please apply your changes to the latest version and try again
I0710 06:48:42.782284       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="default/nodejs-cronjob-28676567"
I0710 06:48:42.785161       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="default/nodejs-cronjob-28676568"
I0710 06:48:42.785913       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="default/nodejs-cronjob-28676566"
I0710 07:08:15.732404       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="91.893388ms"
I0710 07:08:15.766360       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="33.122504ms"
I0710 07:08:15.767132       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="46.084µs"
I0710 07:08:15.781006       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="21.713µs"
I0710 07:08:15.807243       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="30.188µs"
I0710 07:08:20.201152       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="14.555208ms"
I0710 07:08:20.201299       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="34.057µs"
I0710 07:08:24.256523       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="13.707542ms"
I0710 07:08:24.257006       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="36.484µs"
I0710 07:08:36.442423       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="7.386831ms"
I0710 07:08:36.443257       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="50.814µs"
I0710 07:18:28.232573       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="80.5546ms"
I0710 07:18:28.705316       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="472.630164ms"
I0710 07:18:28.760356       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="54.967297ms"
I0710 07:18:28.794324       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="33.671132ms"
I0710 07:18:28.794456       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="37.856µs"
I0710 07:18:28.854081       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="32.705µs"
I0710 07:18:29.063309       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="5.737µs"
I0710 07:21:02.929965       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="75.589157ms"
I0710 07:21:02.990081       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="59.601818ms"
I0710 07:21:03.039883       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="49.761405ms"
I0710 07:21:03.040619       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="47µs"
I0710 07:21:09.045666       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="12.784761ms"
I0710 07:21:09.045807       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="33.454µs"
I0710 07:21:11.084546       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="13.773041ms"
I0710 07:21:11.084744       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="36.116µs"
I0710 07:21:14.135086       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="10.520072ms"
I0710 07:21:14.136099       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="926.71µs"
I0710 07:40:31.112504       1 endpointslice_controller.go:311] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="default/nginx-service" err="EndpointSlice informer cache is out of date"
I0710 07:40:31.133053       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="88.871157ms"
I0710 07:40:31.289537       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="156.448716ms"
I0710 07:40:31.323968       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="34.270274ms"
I0710 07:40:31.324026       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="33.098µs"
I0710 07:40:31.359571       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="31.695µs"
I0710 07:40:31.521010       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="6.412µs"
I0710 07:59:01.078237       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="86.196945ms"
I0710 07:59:01.112561       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="34.039865ms"
I0710 07:59:01.112711       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="37.489µs"
I0710 07:59:01.131938       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="37.459µs"
I0710 07:59:01.155722       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="34.489µs"
I0710 07:59:06.697019       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="18.598909ms"
I0710 07:59:06.697176       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="82.576µs"
I0710 07:59:08.727138       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="13.166713ms"
I0710 07:59:08.727231       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="34.535µs"
I0710 07:59:11.780279       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="16.271028ms"
I0710 07:59:11.781012       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="47.641µs"


==> kube-controller-manager [5351dab4d466] <==
I0710 00:58:19.873038       1 shared_informer.go:320] Caches are synced for TTL after finished
I0710 00:58:19.945159       1 shared_informer.go:320] Caches are synced for PVC protection
I0710 00:58:19.947202       1 shared_informer.go:320] Caches are synced for job
I0710 00:58:19.951037       1 shared_informer.go:320] Caches are synced for ephemeral
I0710 00:58:19.959633       1 shared_informer.go:320] Caches are synced for stateful set
I0710 00:58:19.962634       1 shared_informer.go:320] Caches are synced for cronjob
I0710 00:58:19.970228       1 shared_informer.go:320] Caches are synced for expand
I0710 00:58:19.971339       1 shared_informer.go:320] Caches are synced for persistent volume
I0710 00:58:19.974617       1 shared_informer.go:320] Caches are synced for attach detach
I0710 00:58:19.998473       1 shared_informer.go:320] Caches are synced for resource quota
I0710 00:58:20.008163       1 shared_informer.go:320] Caches are synced for resource quota
I0710 00:58:20.417767       1 shared_informer.go:320] Caches are synced for garbage collector
I0710 00:58:20.417921       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I0710 00:58:20.448178       1 shared_informer.go:320] Caches are synced for garbage collector
I0710 01:52:59.068589       1 replica_set.go:676] "Finished syncing" logger="replicationcontroller-controller" kind="ReplicationController" key="default/nginx-rc" duration="50.220086ms"
I0710 01:52:59.082269       1 replica_set.go:676] "Finished syncing" logger="replicationcontroller-controller" kind="ReplicationController" key="default/nginx-rc" duration="12.199976ms"
I0710 01:52:59.083483       1 replica_set.go:676] "Finished syncing" logger="replicationcontroller-controller" kind="ReplicationController" key="default/nginx-rc" duration="698.607µs"
I0710 01:52:59.085137       1 replica_set.go:676] "Finished syncing" logger="replicationcontroller-controller" kind="ReplicationController" key="default/nginx-rc" duration="78.852µs"
I0710 01:52:59.086813       1 replica_set.go:676] "Finished syncing" logger="replicationcontroller-controller" kind="ReplicationController" key="default/nginx-rc" duration="223.516µs"
I0710 01:52:59.112481       1 replica_set.go:676] "Finished syncing" logger="replicationcontroller-controller" kind="ReplicationController" key="default/nginx-rc" duration="2.869723ms"
I0710 01:52:59.137436       1 replica_set.go:676] "Finished syncing" logger="replicationcontroller-controller" kind="ReplicationController" key="default/nginx-rc" duration="232.224µs"
I0710 01:52:59.151663       1 replica_set.go:676] "Finished syncing" logger="replicationcontroller-controller" kind="ReplicationController" key="default/nginx-rc" duration="63.288µs"
I0710 01:53:06.825094       1 replica_set.go:676] "Finished syncing" logger="replicationcontroller-controller" kind="ReplicationController" key="default/nginx-rc" duration="7.002523ms"
I0710 01:53:06.825329       1 replica_set.go:676] "Finished syncing" logger="replicationcontroller-controller" kind="ReplicationController" key="default/nginx-rc" duration="36.591µs"
I0710 01:53:08.896152       1 replica_set.go:676] "Finished syncing" logger="replicationcontroller-controller" kind="ReplicationController" key="default/nginx-rc" duration="7.356558ms"
I0710 01:53:08.897426       1 replica_set.go:676] "Finished syncing" logger="replicationcontroller-controller" kind="ReplicationController" key="default/nginx-rc" duration="1.048668ms"
I0710 01:53:12.003885       1 replica_set.go:676] "Finished syncing" logger="replicationcontroller-controller" kind="ReplicationController" key="default/nginx-rc" duration="7.649871ms"
I0710 01:53:12.005269       1 replica_set.go:676] "Finished syncing" logger="replicationcontroller-controller" kind="ReplicationController" key="default/nginx-rc" duration="56.412µs"
I0710 01:55:07.841004       1 replica_set.go:676] "Finished syncing" logger="replicationcontroller-controller" kind="ReplicationController" key="default/nginx-rc" duration="315.842649ms"
I0710 01:55:07.888159       1 replica_set.go:676] "Finished syncing" logger="replicationcontroller-controller" kind="ReplicationController" key="default/nginx-rc" duration="47.11146ms"
I0710 01:55:07.888486       1 replica_set.go:676] "Finished syncing" logger="replicationcontroller-controller" kind="ReplicationController" key="default/nginx-rc" duration="52.401µs"
I0710 01:55:07.895243       1 replica_set.go:676] "Finished syncing" logger="replicationcontroller-controller" kind="ReplicationController" key="default/nginx-rc" duration="60.153µs"
I0710 01:55:08.424351       1 replica_set.go:676] "Finished syncing" logger="replicationcontroller-controller" kind="ReplicationController" key="default/nginx-rc" duration="34.592µs"
I0710 01:55:09.326508       1 replica_set.go:676] "Finished syncing" logger="replicationcontroller-controller" kind="ReplicationController" key="default/nginx-rc" duration="34.441µs"
I0710 01:55:09.331379       1 replica_set.go:676] "Finished syncing" logger="replicationcontroller-controller" kind="ReplicationController" key="default/nginx-rc" duration="38.409µs"
I0710 01:55:12.479197       1 replica_set.go:676] "Finished syncing" logger="replicationcontroller-controller" kind="ReplicationController" key="default/nginx-rc" duration="11.384392ms"
I0710 01:55:12.479866       1 replica_set.go:676] "Finished syncing" logger="replicationcontroller-controller" kind="ReplicationController" key="default/nginx-rc" duration="350.045µs"
I0710 03:53:35.459381       1 namespace_controller.go:182] "Namespace has been deleted" logger="namespace-controller" namespace="finance"
I0710 04:11:53.799093       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="110.658102ms"
I0710 04:11:53.812808       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="13.543373ms"
I0710 04:11:53.832569       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="19.664483ms"
I0710 04:11:53.833052       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="44.273µs"
I0710 04:11:59.188146       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="12.610207ms"
I0710 04:11:59.189651       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="69.396µs"
I0710 04:12:02.259007       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="6.79245ms"
I0710 04:12:02.261865       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="46.623µs"
I0710 04:12:07.410408       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="7.305162ms"
I0710 04:12:07.410746       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="43.656µs"
I0710 04:17:51.211119       1 replica_set.go:676] "Finished syncing" logger="replicationcontroller-controller" kind="ReplicationController" key="default/nginx-rc" duration="9.086µs"
I0710 04:23:02.120574       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="63.318459ms"
I0710 04:23:02.138288       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="17.552889ms"
I0710 04:23:02.139963       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="169.997µs"
I0710 04:23:02.159386       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="31.732µs"
I0710 04:23:02.636772       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="38.101µs"
I0710 04:23:02.699738       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="32.91µs"
I0710 04:23:03.021553       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="38.999µs"
I0710 04:23:03.032774       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="28.955µs"
I0710 04:23:08.160457       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="6.779924ms"
I0710 04:23:08.161110       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="43.222µs"
I0710 04:41:43.642666       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx" duration="7.789µs"


==> kube-proxy [0ca8230c062c] <==
I0710 05:34:26.244635       1 server_linux.go:69] "Using iptables proxy"
I0710 05:34:26.368571       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
I0710 05:34:26.597178       1 server.go:659] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0710 05:34:26.597901       1 server_linux.go:165] "Using iptables Proxier"
I0710 05:34:26.614070       1 server_linux.go:511] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0710 05:34:26.614125       1 server_linux.go:528] "Defaulting to no-op detect-local"
I0710 05:34:26.615249       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0710 05:34:26.615856       1 server.go:872] "Version info" version="v1.30.0"
I0710 05:34:26.615893       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0710 05:34:26.645186       1 config.go:192] "Starting service config controller"
I0710 05:34:26.673199       1 shared_informer.go:313] Waiting for caches to sync for service config
I0710 05:34:26.673310       1 shared_informer.go:320] Caches are synced for service config
I0710 05:34:26.649247       1 config.go:319] "Starting node config controller"
I0710 05:34:26.674351       1 shared_informer.go:313] Waiting for caches to sync for node config
I0710 05:34:26.647683       1 config.go:101] "Starting endpoint slice config controller"
I0710 05:34:26.674388       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0710 05:34:26.775793       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0710 05:34:26.776440       1 shared_informer.go:320] Caches are synced for node config


==> kube-proxy [322116d6818f] <==
I0710 00:58:10.635402       1 server_linux.go:69] "Using iptables proxy"
I0710 00:58:10.702748       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
I0710 00:58:10.824936       1 server.go:659] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0710 00:58:10.825019       1 server_linux.go:165] "Using iptables Proxier"
I0710 00:58:10.830369       1 server_linux.go:511] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0710 00:58:10.830438       1 server_linux.go:528] "Defaulting to no-op detect-local"
I0710 00:58:10.833465       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0710 00:58:10.837342       1 server.go:872] "Version info" version="v1.30.0"
I0710 00:58:10.837393       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0710 00:58:10.843729       1 config.go:192] "Starting service config controller"
I0710 00:58:10.843810       1 shared_informer.go:313] Waiting for caches to sync for service config
I0710 00:58:10.845392       1 config.go:101] "Starting endpoint slice config controller"
I0710 00:58:10.845401       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0710 00:58:10.852204       1 config.go:319] "Starting node config controller"
I0710 00:58:10.852418       1 shared_informer.go:313] Waiting for caches to sync for node config
I0710 00:58:10.946772       1 shared_informer.go:320] Caches are synced for service config
I0710 00:58:10.946708       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0710 00:58:10.953615       1 shared_informer.go:320] Caches are synced for node config


==> kube-scheduler [15ad977ace55] <==
I0710 05:34:19.087804       1 serving.go:380] Generated self-signed cert in-memory
W0710 05:34:23.320105       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0710 05:34:23.321463       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0710 05:34:23.326336       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0710 05:34:23.326653       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0710 05:34:23.433289       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.30.0"
I0710 05:34:23.433442       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0710 05:34:23.435250       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0710 05:34:23.436277       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0710 05:34:23.436345       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0710 05:34:23.436354       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0710 05:34:23.537518       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kube-scheduler [8ec4a256cecd] <==
I0710 00:58:03.208872       1 serving.go:380] Generated self-signed cert in-memory
W0710 00:58:07.123541       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0710 00:58:07.123748       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0710 00:58:07.124024       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0710 00:58:07.124031       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0710 00:58:07.212734       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.30.0"
I0710 00:58:07.212795       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0710 00:58:07.222870       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0710 00:58:07.226859       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0710 00:58:07.226899       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0710 00:58:07.233549       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0710 00:58:07.334079       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0710 04:50:32.455610       1 configmap_cafile_content.go:223] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0710 04:50:32.415119       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
I0710 04:50:32.463196       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
E0710 04:50:32.623631       1 run.go:74] "command failed" err="finished without leader elect"


==> kubelet <==
Jul 10 07:40:32 minikube kubelet[1215]: I0710 07:40:32.696865    1215 reconciler_common.go:289] "Volume detached for volume \"kube-api-access-jxxhd\" (UniqueName: \"kubernetes.io/projected/3c8a231d-906c-412c-816f-5ee177188ca4-kube-api-access-jxxhd\") on node \"minikube\" DevicePath \"\""
Jul 10 07:40:32 minikube kubelet[1215]: I0710 07:40:32.696951    1215 reconciler_common.go:289] "Volume detached for volume \"kube-api-access-dnbld\" (UniqueName: \"kubernetes.io/projected/b0b6aeed-c7a5-4a85-a382-348f58e7f5ee-kube-api-access-dnbld\") on node \"minikube\" DevicePath \"\""
Jul 10 07:40:32 minikube kubelet[1215]: I0710 07:40:32.797265    1215 reconciler_common.go:161] "operationExecutor.UnmountVolume started for volume \"kube-api-access-kmhhh\" (UniqueName: \"kubernetes.io/projected/a099a114-36d9-4fa8-bc09-cdd5791be7b1-kube-api-access-kmhhh\") pod \"a099a114-36d9-4fa8-bc09-cdd5791be7b1\" (UID: \"a099a114-36d9-4fa8-bc09-cdd5791be7b1\") "
Jul 10 07:40:32 minikube kubelet[1215]: I0710 07:40:32.802948    1215 operation_generator.go:887] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/a099a114-36d9-4fa8-bc09-cdd5791be7b1-kube-api-access-kmhhh" (OuterVolumeSpecName: "kube-api-access-kmhhh") pod "a099a114-36d9-4fa8-bc09-cdd5791be7b1" (UID: "a099a114-36d9-4fa8-bc09-cdd5791be7b1"). InnerVolumeSpecName "kube-api-access-kmhhh". PluginName "kubernetes.io/projected", VolumeGidValue ""
Jul 10 07:40:32 minikube kubelet[1215]: I0710 07:40:32.898316    1215 reconciler_common.go:289] "Volume detached for volume \"kube-api-access-kmhhh\" (UniqueName: \"kubernetes.io/projected/a099a114-36d9-4fa8-bc09-cdd5791be7b1-kube-api-access-kmhhh\") on node \"minikube\" DevicePath \"\""
Jul 10 07:40:33 minikube kubelet[1215]: I0710 07:40:33.682274    1215 scope.go:117] "RemoveContainer" containerID="df7cee9e7ffff8c430b8f5ae304e95303cd7e0bbd5e6196db973d65d68bf0a56"
Jul 10 07:40:33 minikube kubelet[1215]: I0710 07:40:33.733973    1215 scope.go:117] "RemoveContainer" containerID="30496464776becd30597920fc06dfc1a3f9bf187ab7fe11f254d0947aa60ddac"
Jul 10 07:40:33 minikube kubelet[1215]: I0710 07:40:33.966160    1215 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="0120f70c-cabe-4192-947a-86453ca20066" path="/var/lib/kubelet/pods/0120f70c-cabe-4192-947a-86453ca20066/volumes"
Jul 10 07:40:33 minikube kubelet[1215]: I0710 07:40:33.966858    1215 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="07700836-c853-4653-8fb2-c84520452c01" path="/var/lib/kubelet/pods/07700836-c853-4653-8fb2-c84520452c01/volumes"
Jul 10 07:40:33 minikube kubelet[1215]: I0710 07:40:33.967119    1215 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="3c8a231d-906c-412c-816f-5ee177188ca4" path="/var/lib/kubelet/pods/3c8a231d-906c-412c-816f-5ee177188ca4/volumes"
Jul 10 07:40:33 minikube kubelet[1215]: I0710 07:40:33.967403    1215 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="a099a114-36d9-4fa8-bc09-cdd5791be7b1" path="/var/lib/kubelet/pods/a099a114-36d9-4fa8-bc09-cdd5791be7b1/volumes"
Jul 10 07:40:33 minikube kubelet[1215]: I0710 07:40:33.967547    1215 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="b0b6aeed-c7a5-4a85-a382-348f58e7f5ee" path="/var/lib/kubelet/pods/b0b6aeed-c7a5-4a85-a382-348f58e7f5ee/volumes"
Jul 10 07:40:38 minikube kubelet[1215]: I0710 07:40:38.923779    1215 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/nginx-ws4xs" podStartSLOduration=5.050661085 podStartE2EDuration="7.923762315s" podCreationTimestamp="2024-07-10 07:40:31 +0000 UTC" firstStartedPulling="2024-07-10 07:40:32.740363945 +0000 UTC m=+7579.968651966" lastFinishedPulling="2024-07-10 07:40:35.613465174 +0000 UTC m=+7582.841753196" observedRunningTime="2024-07-10 07:40:35.888261738 +0000 UTC m=+7583.116549770" watchObservedRunningTime="2024-07-10 07:40:38.923762315 +0000 UTC m=+7586.152050344"
Jul 10 07:40:39 minikube kubelet[1215]: I0710 07:40:39.464354    1215 reconciler_common.go:161] "operationExecutor.UnmountVolume started for volume \"kube-api-access-xt8zx\" (UniqueName: \"kubernetes.io/projected/7543fe4c-6708-452c-a6cc-0d994a724387-kube-api-access-xt8zx\") pod \"7543fe4c-6708-452c-a6cc-0d994a724387\" (UID: \"7543fe4c-6708-452c-a6cc-0d994a724387\") "
Jul 10 07:40:39 minikube kubelet[1215]: I0710 07:40:39.469606    1215 operation_generator.go:887] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/7543fe4c-6708-452c-a6cc-0d994a724387-kube-api-access-xt8zx" (OuterVolumeSpecName: "kube-api-access-xt8zx") pod "7543fe4c-6708-452c-a6cc-0d994a724387" (UID: "7543fe4c-6708-452c-a6cc-0d994a724387"). InnerVolumeSpecName "kube-api-access-xt8zx". PluginName "kubernetes.io/projected", VolumeGidValue ""
Jul 10 07:40:39 minikube kubelet[1215]: I0710 07:40:39.565113    1215 reconciler_common.go:289] "Volume detached for volume \"kube-api-access-xt8zx\" (UniqueName: \"kubernetes.io/projected/7543fe4c-6708-452c-a6cc-0d994a724387-kube-api-access-xt8zx\") on node \"minikube\" DevicePath \"\""
Jul 10 07:40:39 minikube kubelet[1215]: I0710 07:40:39.911618    1215 scope.go:117] "RemoveContainer" containerID="3998f50ce791bb49ca98b4a97799a43aa7d8d4b0d9a4c61c36e5ade290c0d6c0"
Jul 10 07:40:39 minikube kubelet[1215]: I0710 07:40:39.943622    1215 scope.go:117] "RemoveContainer" containerID="3998f50ce791bb49ca98b4a97799a43aa7d8d4b0d9a4c61c36e5ade290c0d6c0"
Jul 10 07:40:39 minikube kubelet[1215]: E0710 07:40:39.944898    1215 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 3998f50ce791bb49ca98b4a97799a43aa7d8d4b0d9a4c61c36e5ade290c0d6c0" containerID="3998f50ce791bb49ca98b4a97799a43aa7d8d4b0d9a4c61c36e5ade290c0d6c0"
Jul 10 07:40:39 minikube kubelet[1215]: I0710 07:40:39.944961    1215 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"3998f50ce791bb49ca98b4a97799a43aa7d8d4b0d9a4c61c36e5ade290c0d6c0"} err="failed to get container status \"3998f50ce791bb49ca98b4a97799a43aa7d8d4b0d9a4c61c36e5ade290c0d6c0\": rpc error: code = Unknown desc = Error response from daemon: No such container: 3998f50ce791bb49ca98b4a97799a43aa7d8d4b0d9a4c61c36e5ade290c0d6c0"
Jul 10 07:40:41 minikube kubelet[1215]: I0710 07:40:41.965391    1215 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="7543fe4c-6708-452c-a6cc-0d994a724387" path="/var/lib/kubelet/pods/7543fe4c-6708-452c-a6cc-0d994a724387/volumes"
Jul 10 07:41:06 minikube kubelet[1215]: I0710 07:41:06.565097    1215 reconciler_common.go:161] "operationExecutor.UnmountVolume started for volume \"kube-api-access-m972q\" (UniqueName: \"kubernetes.io/projected/7fcc4b95-8153-494f-aa64-4cdbb37c58f8-kube-api-access-m972q\") pod \"7fcc4b95-8153-494f-aa64-4cdbb37c58f8\" (UID: \"7fcc4b95-8153-494f-aa64-4cdbb37c58f8\") "
Jul 10 07:41:06 minikube kubelet[1215]: I0710 07:41:06.572001    1215 operation_generator.go:887] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/7fcc4b95-8153-494f-aa64-4cdbb37c58f8-kube-api-access-m972q" (OuterVolumeSpecName: "kube-api-access-m972q") pod "7fcc4b95-8153-494f-aa64-4cdbb37c58f8" (UID: "7fcc4b95-8153-494f-aa64-4cdbb37c58f8"). InnerVolumeSpecName "kube-api-access-m972q". PluginName "kubernetes.io/projected", VolumeGidValue ""
Jul 10 07:41:06 minikube kubelet[1215]: I0710 07:41:06.665410    1215 reconciler_common.go:289] "Volume detached for volume \"kube-api-access-m972q\" (UniqueName: \"kubernetes.io/projected/7fcc4b95-8153-494f-aa64-4cdbb37c58f8-kube-api-access-m972q\") on node \"minikube\" DevicePath \"\""
Jul 10 07:41:07 minikube kubelet[1215]: I0710 07:41:07.298230    1215 scope.go:117] "RemoveContainer" containerID="09d2acbcc87a39dcd30b9a0a81b578b48512eebc0d5a0a0a90341a3bd9d0483d"
Jul 10 07:41:07 minikube kubelet[1215]: I0710 07:41:07.964091    1215 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="7fcc4b95-8153-494f-aa64-4cdbb37c58f8" path="/var/lib/kubelet/pods/7fcc4b95-8153-494f-aa64-4cdbb37c58f8/volumes"
Jul 10 07:41:18 minikube kubelet[1215]: I0710 07:41:18.402866    1215 topology_manager.go:215] "Topology Admit Handler" podUID="d6eb4b7c-1c77-4d01-9258-5069f1fbecd1" podNamespace="default" podName="curl"
Jul 10 07:41:18 minikube kubelet[1215]: E0710 07:41:18.402911    1215 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="3c8a231d-906c-412c-816f-5ee177188ca4" containerName="nginx"
Jul 10 07:41:18 minikube kubelet[1215]: E0710 07:41:18.402919    1215 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="07700836-c853-4653-8fb2-c84520452c01" containerName="curl"
Jul 10 07:41:18 minikube kubelet[1215]: E0710 07:41:18.402924    1215 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="0120f70c-cabe-4192-947a-86453ca20066" containerName="nginx"
Jul 10 07:41:18 minikube kubelet[1215]: E0710 07:41:18.402928    1215 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="7fcc4b95-8153-494f-aa64-4cdbb37c58f8" containerName="nginx"
Jul 10 07:41:18 minikube kubelet[1215]: E0710 07:41:18.402932    1215 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="b0b6aeed-c7a5-4a85-a382-348f58e7f5ee" containerName="nginx"
Jul 10 07:41:18 minikube kubelet[1215]: E0710 07:41:18.402936    1215 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="7543fe4c-6708-452c-a6cc-0d994a724387" containerName="nginx"
Jul 10 07:41:18 minikube kubelet[1215]: I0710 07:41:18.402951    1215 memory_manager.go:354] "RemoveStaleState removing state" podUID="7543fe4c-6708-452c-a6cc-0d994a724387" containerName="nginx"
Jul 10 07:41:18 minikube kubelet[1215]: I0710 07:41:18.402955    1215 memory_manager.go:354] "RemoveStaleState removing state" podUID="3c8a231d-906c-412c-816f-5ee177188ca4" containerName="nginx"
Jul 10 07:41:18 minikube kubelet[1215]: I0710 07:41:18.402959    1215 memory_manager.go:354] "RemoveStaleState removing state" podUID="0120f70c-cabe-4192-947a-86453ca20066" containerName="nginx"
Jul 10 07:41:18 minikube kubelet[1215]: I0710 07:41:18.402962    1215 memory_manager.go:354] "RemoveStaleState removing state" podUID="b0b6aeed-c7a5-4a85-a382-348f58e7f5ee" containerName="nginx"
Jul 10 07:41:18 minikube kubelet[1215]: I0710 07:41:18.402965    1215 memory_manager.go:354] "RemoveStaleState removing state" podUID="7fcc4b95-8153-494f-aa64-4cdbb37c58f8" containerName="nginx"
Jul 10 07:41:18 minikube kubelet[1215]: I0710 07:41:18.402969    1215 memory_manager.go:354] "RemoveStaleState removing state" podUID="07700836-c853-4653-8fb2-c84520452c01" containerName="curl"
Jul 10 07:41:18 minikube kubelet[1215]: I0710 07:41:18.569158    1215 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-xcnfm\" (UniqueName: \"kubernetes.io/projected/d6eb4b7c-1c77-4d01-9258-5069f1fbecd1-kube-api-access-xcnfm\") pod \"curl\" (UID: \"d6eb4b7c-1c77-4d01-9258-5069f1fbecd1\") " pod="default/curl"
Jul 10 07:47:54 minikube kubelet[1215]: E0710 07:47:54.663950    1215 upgradeaware.go:427] Error proxying data from client to backend: readfrom tcp [::1]:34410->[::1]:39139: write tcp [::1]:34410->[::1]:39139: write: broken pipe
Jul 10 07:56:05 minikube kubelet[1215]: I0710 07:56:05.364826    1215 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/curl" podStartSLOduration=884.806658439 podStartE2EDuration="14m47.36479427s" podCreationTimestamp="2024-07-10 07:41:18 +0000 UTC" firstStartedPulling="2024-07-10 07:41:19.200328965 +0000 UTC m=+7626.428616989" lastFinishedPulling="2024-07-10 07:41:21.758464795 +0000 UTC m=+7628.986752820" observedRunningTime="2024-07-10 07:41:22.617775088 +0000 UTC m=+7629.846063117" watchObservedRunningTime="2024-07-10 07:56:05.36479427 +0000 UTC m=+8512.593082298"
Jul 10 07:56:05 minikube kubelet[1215]: I0710 07:56:05.861593    1215 reconciler_common.go:161] "operationExecutor.UnmountVolume started for volume \"kube-api-access-xcnfm\" (UniqueName: \"kubernetes.io/projected/d6eb4b7c-1c77-4d01-9258-5069f1fbecd1-kube-api-access-xcnfm\") pod \"d6eb4b7c-1c77-4d01-9258-5069f1fbecd1\" (UID: \"d6eb4b7c-1c77-4d01-9258-5069f1fbecd1\") "
Jul 10 07:56:05 minikube kubelet[1215]: I0710 07:56:05.870400    1215 operation_generator.go:887] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/d6eb4b7c-1c77-4d01-9258-5069f1fbecd1-kube-api-access-xcnfm" (OuterVolumeSpecName: "kube-api-access-xcnfm") pod "d6eb4b7c-1c77-4d01-9258-5069f1fbecd1" (UID: "d6eb4b7c-1c77-4d01-9258-5069f1fbecd1"). InnerVolumeSpecName "kube-api-access-xcnfm". PluginName "kubernetes.io/projected", VolumeGidValue ""
Jul 10 07:56:05 minikube kubelet[1215]: I0710 07:56:05.963770    1215 reconciler_common.go:289] "Volume detached for volume \"kube-api-access-xcnfm\" (UniqueName: \"kubernetes.io/projected/d6eb4b7c-1c77-4d01-9258-5069f1fbecd1-kube-api-access-xcnfm\") on node \"minikube\" DevicePath \"\""
Jul 10 07:56:06 minikube kubelet[1215]: I0710 07:56:06.064509    1215 scope.go:117] "RemoveContainer" containerID="cb233d8b9ab1b26d175e0daea9cf00ab1fcd3ed60cd3b3b356ec14c0c0b4129a"
Jul 10 07:56:06 minikube kubelet[1215]: I0710 07:56:06.110428    1215 scope.go:117] "RemoveContainer" containerID="cb233d8b9ab1b26d175e0daea9cf00ab1fcd3ed60cd3b3b356ec14c0c0b4129a"
Jul 10 07:56:06 minikube kubelet[1215]: E0710 07:56:06.117957    1215 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: cb233d8b9ab1b26d175e0daea9cf00ab1fcd3ed60cd3b3b356ec14c0c0b4129a" containerID="cb233d8b9ab1b26d175e0daea9cf00ab1fcd3ed60cd3b3b356ec14c0c0b4129a"
Jul 10 07:56:06 minikube kubelet[1215]: I0710 07:56:06.118119    1215 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"cb233d8b9ab1b26d175e0daea9cf00ab1fcd3ed60cd3b3b356ec14c0c0b4129a"} err="failed to get container status \"cb233d8b9ab1b26d175e0daea9cf00ab1fcd3ed60cd3b3b356ec14c0c0b4129a\": rpc error: code = Unknown desc = Error response from daemon: No such container: cb233d8b9ab1b26d175e0daea9cf00ab1fcd3ed60cd3b3b356ec14c0c0b4129a"
Jul 10 07:56:07 minikube kubelet[1215]: I0710 07:56:07.967427    1215 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="d6eb4b7c-1c77-4d01-9258-5069f1fbecd1" path="/var/lib/kubelet/pods/d6eb4b7c-1c77-4d01-9258-5069f1fbecd1/volumes"
Jul 10 07:59:01 minikube kubelet[1215]: I0710 07:59:01.029078    1215 topology_manager.go:215] "Topology Admit Handler" podUID="694d84de-4546-408e-bebb-5cc5c0219285" podNamespace="default" podName="nginx-48t55"
Jul 10 07:59:01 minikube kubelet[1215]: E0710 07:59:01.029775    1215 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="d6eb4b7c-1c77-4d01-9258-5069f1fbecd1" containerName="curl"
Jul 10 07:59:01 minikube kubelet[1215]: I0710 07:59:01.029810    1215 memory_manager.go:354] "RemoveStaleState removing state" podUID="d6eb4b7c-1c77-4d01-9258-5069f1fbecd1" containerName="curl"
Jul 10 07:59:01 minikube kubelet[1215]: I0710 07:59:01.077619    1215 topology_manager.go:215] "Topology Admit Handler" podUID="56544484-8919-462b-a027-c3b1acf40761" podNamespace="default" podName="nginx-ghhqn"
Jul 10 07:59:01 minikube kubelet[1215]: I0710 07:59:01.092932    1215 topology_manager.go:215] "Topology Admit Handler" podUID="c18a97db-12ce-48f8-a9a3-0fd65fcac4a6" podNamespace="default" podName="nginx-ncp8b"
Jul 10 07:59:01 minikube kubelet[1215]: I0710 07:59:01.135531    1215 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-ftxqn\" (UniqueName: \"kubernetes.io/projected/694d84de-4546-408e-bebb-5cc5c0219285-kube-api-access-ftxqn\") pod \"nginx-48t55\" (UID: \"694d84de-4546-408e-bebb-5cc5c0219285\") " pod="default/nginx-48t55"
Jul 10 07:59:01 minikube kubelet[1215]: I0710 07:59:01.238440    1215 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-b89cm\" (UniqueName: \"kubernetes.io/projected/c18a97db-12ce-48f8-a9a3-0fd65fcac4a6-kube-api-access-b89cm\") pod \"nginx-ncp8b\" (UID: \"c18a97db-12ce-48f8-a9a3-0fd65fcac4a6\") " pod="default/nginx-ncp8b"
Jul 10 07:59:01 minikube kubelet[1215]: I0710 07:59:01.238479    1215 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-clxhh\" (UniqueName: \"kubernetes.io/projected/56544484-8919-462b-a027-c3b1acf40761-kube-api-access-clxhh\") pod \"nginx-ghhqn\" (UID: \"56544484-8919-462b-a027-c3b1acf40761\") " pod="default/nginx-ghhqn"
Jul 10 07:59:08 minikube kubelet[1215]: I0710 07:59:08.711969    1215 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/nginx-48t55" podStartSLOduration=4.703211087 podStartE2EDuration="7.711952334s" podCreationTimestamp="2024-07-10 07:59:01 +0000 UTC" firstStartedPulling="2024-07-10 07:59:02.361850186 +0000 UTC m=+8689.590138205" lastFinishedPulling="2024-07-10 07:59:05.37059143 +0000 UTC m=+8692.598879452" observedRunningTime="2024-07-10 07:59:06.677810554 +0000 UTC m=+8693.906098590" watchObservedRunningTime="2024-07-10 07:59:08.711952334 +0000 UTC m=+8695.940240364"
Jul 10 07:59:11 minikube kubelet[1215]: I0710 07:59:11.762865    1215 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/nginx-ghhqn" podStartSLOduration=4.873517738 podStartE2EDuration="10.762846688s" podCreationTimestamp="2024-07-10 07:59:01 +0000 UTC" firstStartedPulling="2024-07-10 07:59:02.389465069 +0000 UTC m=+8689.617753089" lastFinishedPulling="2024-07-10 07:59:08.278794014 +0000 UTC m=+8695.507082039" observedRunningTime="2024-07-10 07:59:08.712882293 +0000 UTC m=+8695.941170316" watchObservedRunningTime="2024-07-10 07:59:11.762846688 +0000 UTC m=+8698.991134716"


==> storage-provisioner [a1930c316820] <==
I0710 05:34:25.815562       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0710 05:34:55.840673       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout


==> storage-provisioner [ebbf7ae3be7a] <==
I0710 05:35:08.221783       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0710 05:35:08.260342       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0710 05:35:08.264244       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0710 05:35:25.745006       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0710 05:35:25.745199       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_af16b037-1598-4a36-869f-987531188eeb!
I0710 05:35:25.746009       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"00df3889-83d3-477e-b176-5e24fc943328", APIVersion:"v1", ResourceVersion:"52361", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_af16b037-1598-4a36-869f-987531188eeb became leader
I0710 05:35:25.851770       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_af16b037-1598-4a36-869f-987531188eeb!

